{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Sentiment analysis using NaiveBayes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulkhankar/SelfProject/blob/master/Sentiment_analysis_using_NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmPwPQD6_LEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFTawOBw_LEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1187b71d-9c83-46a3-908e-1b638d5f99b0"
      },
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSnOfbma_LEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import twitter_samples"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj7UJ9Wh_LEu",
        "colab_type": "text"
      },
      "source": [
        "This will import three datasets from NLTK that contain various tweets to train and test the model:\n",
        "\n",
        "negative_tweets.json: 5000 tweets with negative sentiments \n",
        "\n",
        "positive_tweets.json: 5000 tweets with positive sentiments \n",
        "\n",
        "tweets.20150430-223406.json: 20000 tweets with no sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lGBriU4_LEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWcsGvYj_LEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#positive_tweets[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsifRB3Q_LE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#negative_tweets[0]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKhXmAL0_LE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGBC3lBK_LE4",
        "colab_type": "text"
      },
      "source": [
        "The punkt module is a pre-trained model that will help tokenize words and sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RpX1Dze_LE5",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gLazonY_LE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXw4lg-6_LE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04ae4f50-a3d1-481b-a543-16f0dd048152"
      },
      "source": [
        "print(tweet_tokens[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOuR4rKw_LE_",
        "colab_type": "text"
      },
      "source": [
        "# Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR-Ux62S_LFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('wordnet')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJbBfsGi_LFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxH7TVXQ_LFF",
        "colab_type": "text"
      },
      "source": [
        "wordnet is a lexical database for the English language that helps the script determine the base word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R872FWTt_LFF",
        "colab_type": "text"
      },
      "source": [
        "averaged_perceptron_tagger resource to determine the context of a word in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbztdVJb_LFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZDuPoek_LFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc03cea9-06b9-41eb-c725-a4e44216aee5"
      },
      "source": [
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRGFp9Ra_LFJ",
        "colab_type": "text"
      },
      "source": [
        "pos_tag function provide a list of tokens as an argument to get the tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azy6zj1C_LFK",
        "colab_type": "text"
      },
      "source": [
        "CC\tCoordinating conjunction\n",
        "\n",
        "CD\tCardinal number\n",
        "\n",
        "DT\tDeterminer\n",
        "\n",
        "EX\tExistential there\n",
        "\n",
        "FW\tForeign word\n",
        "\n",
        "IN\tPreposition or subordinating conjunction\n",
        "\n",
        "JJ\tAdjective\n",
        "\n",
        "JJR\tAdjective, comparative\n",
        "\n",
        "JJS\tAdjective, superlative\n",
        "\n",
        "1LS\tList item marker\n",
        "\n",
        "MD\tModal\n",
        "\n",
        "NN\tNoun, singular or mass\n",
        "\n",
        "NNS\tNoun, plural\n",
        "\n",
        "NNP\tProper noun, singular\n",
        "\n",
        "NNPS\tProper noun, plural\n",
        "\n",
        "PDT\tPredeterminer\n",
        "\n",
        "POS\tPossessive ending\n",
        "\n",
        "PRP\tPersonal pronoun\n",
        "\n",
        "PRP$\tPossessive pronoun\n",
        "\n",
        "RB\tAdverb\n",
        "\n",
        "RBR\tAdverb, comparative\n",
        "\n",
        "RBS\tAdverb, superlative\n",
        "\n",
        "RP\tParticle\n",
        "\n",
        "SYM\tSymbol\n",
        "\n",
        "TO\tto\n",
        "\n",
        "UH\tInterjection\n",
        "\n",
        "VB\tVerb, base form\n",
        "\n",
        "VBD\tVerb, past tense\n",
        "\n",
        "VBG\tVerb, gerund or present participle\n",
        "\n",
        "VBN\tVerb, past participle\n",
        "\n",
        "VBP\tVerb, non-3rd person singular present\n",
        "\n",
        "VBZ\tVerb, 3rd person singular present\n",
        "\n",
        "WDT\tWh-determiner\n",
        "\n",
        "WP\tWh-pronoun\n",
        "\n",
        "WP$\tPossessive wh-pronoun\n",
        "\n",
        "WRB\tWh-adverb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_2mCj5w_LFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47VqzV8R_LFN",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, the same word can have a multiple lemmas based on the meaning / context. hence we will also incorporate part of speech when lemmatizing sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM_NBvTq_LFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return (lemmatized_sentence)\n",
        "\n",
        "print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSWx7DPt_LFQ",
        "colab_type": "text"
      },
      "source": [
        "# Removing Noise from the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMIBuDS3_LFR",
        "colab_type": "text"
      },
      "source": [
        "Hyperlinks, Twitter usernames are preceded by a @ symbol, Punctuation and special characters\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZmaBM4S_LFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, string"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H34VFGWkAXgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtkBkM4G_LFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "    \n",
        "    cleaned_tokens = []\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)  #This is for Hyperlinks\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)               #This is for usernames staring with @\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:  #remove punctuation using the library string\n",
        "            cleaned_tokens.append(token.lower())\n",
        "        \n",
        "    return (cleaned_tokens)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbpgiqEp_LFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('stopwords')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqy7xCNI_LFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from nltk.corpus import stopwords\n",
        "#stop_words = stopwords.words('english')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZO_m7br_LFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(stop_words)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtLGIOjfAeBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "62350667-fbf5-403f-8cb3-97ad2ca2deb6"
      },
      "source": [
        "#nltk.download('wordnet')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OajUc2u1_LFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before','to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than','s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should','now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'ma', \"weren't\", 'won']"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZh5_ZBd_pih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfJCIw-s_LFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b24b4b68-a730-40f6-fd61-17fc27548b97"
      },
      "source": [
        "print(remove_noise(tweet_tokens[0], stop_words))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFAzXTTI_LFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MATDsu7v_LFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVF7qB1H_LFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwqMsx90_LFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "071c98dc-49f3-432b-b6b6-169848ac72a4"
      },
      "source": [
        "print('Original:',positive_tweet_tokens[500])\n",
        "print('\\n')\n",
        "print('Cleaned:',positive_cleaned_tokens_list[500])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: ['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
            "\n",
            "\n",
            "Cleaned: ['dang', 'rad', '#fanart', ':d']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osJeX219_LFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqnIKd99_LFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M7dFpu7_LFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_neg_words = get_all_words(negative_cleaned_tokens_list)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3GCY4zI_LFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_words=[]\n",
        "for i in all_pos_words:\n",
        "    positive_words.append(i)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMFIoTDx_LF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_words=[]\n",
        "for i in all_neg_words:\n",
        "    negative_words.append(i)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLxQdikH_LF3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9de3dba0-36ca-47c5-d9a8-5c9dcc35e541"
      },
      "source": [
        "positive_words"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#followfriday',\n",
              " 'top',\n",
              " 'engage',\n",
              " 'member',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'hey',\n",
              " 'james',\n",
              " 'odd',\n",
              " ':/',\n",
              " 'please',\n",
              " 'call',\n",
              " 'contact',\n",
              " 'centre',\n",
              " '02392441234',\n",
              " 'able',\n",
              " 'assist',\n",
              " ':)',\n",
              " 'many',\n",
              " 'thanks',\n",
              " 'listen',\n",
              " 'last',\n",
              " 'night',\n",
              " ':)',\n",
              " 'bleed',\n",
              " 'amazing',\n",
              " 'track',\n",
              " 'scotland',\n",
              " 'congrats',\n",
              " ':)',\n",
              " 'yeaaaah',\n",
              " 'yippppy',\n",
              " 'accnt',\n",
              " 'verify',\n",
              " 'rqst',\n",
              " 'succeed',\n",
              " 'get',\n",
              " 'blue',\n",
              " 'tick',\n",
              " 'mark',\n",
              " 'fb',\n",
              " 'profile',\n",
              " ':)',\n",
              " '15',\n",
              " 'day',\n",
              " 'one',\n",
              " 'irresistible',\n",
              " ':)',\n",
              " '#flipkartfashionfriday',\n",
              " 'like',\n",
              " 'keep',\n",
              " 'lovely',\n",
              " 'customer',\n",
              " 'wait',\n",
              " 'long',\n",
              " 'hope',\n",
              " 'enjoy',\n",
              " 'happy',\n",
              " 'friday',\n",
              " 'lwwf',\n",
              " ':)',\n",
              " 'second',\n",
              " 'thought',\n",
              " '’',\n",
              " 'not',\n",
              " 'enough',\n",
              " 'time',\n",
              " 'dd',\n",
              " ':)',\n",
              " 'new',\n",
              " 'short',\n",
              " 'enter',\n",
              " 'system',\n",
              " 'sheep',\n",
              " 'must',\n",
              " 'buy',\n",
              " 'jgh',\n",
              " 'go',\n",
              " 'bayan',\n",
              " ':d',\n",
              " 'bye',\n",
              " 'act',\n",
              " 'mischievousness',\n",
              " 'call',\n",
              " 'etl',\n",
              " 'layer',\n",
              " 'in-house',\n",
              " 'warehouse',\n",
              " 'app',\n",
              " 'katamari',\n",
              " 'well',\n",
              " '…',\n",
              " 'name',\n",
              " 'imply',\n",
              " ':p',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'influencers',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " \"wouldn't\",\n",
              " 'love',\n",
              " 'big',\n",
              " '...',\n",
              " 'juicy',\n",
              " '...',\n",
              " 'selfies',\n",
              " ':)',\n",
              " 'follow',\n",
              " 'follow',\n",
              " 'u',\n",
              " 'back',\n",
              " ':)',\n",
              " 'perfect',\n",
              " 'already',\n",
              " 'know',\n",
              " \"what's\",\n",
              " 'wait',\n",
              " ':)',\n",
              " 'great',\n",
              " 'new',\n",
              " 'opportunity',\n",
              " 'junior',\n",
              " 'triathletes',\n",
              " 'age',\n",
              " '12',\n",
              " '13',\n",
              " 'gatorade',\n",
              " 'series',\n",
              " 'get',\n",
              " 'entry',\n",
              " ':)',\n",
              " 'laying',\n",
              " 'greeting',\n",
              " 'card',\n",
              " 'range',\n",
              " 'print',\n",
              " 'today',\n",
              " 'love',\n",
              " 'job',\n",
              " ':-)',\n",
              " \"friend's\",\n",
              " 'lunch',\n",
              " '...',\n",
              " 'yummmm',\n",
              " ':)',\n",
              " '#nostalgia',\n",
              " '#tbs',\n",
              " '#ku',\n",
              " 'id',\n",
              " 'conflict',\n",
              " 'thanks',\n",
              " 'help',\n",
              " ':d',\n",
              " \"here's\",\n",
              " 'screenshot',\n",
              " 'work',\n",
              " 'hi',\n",
              " 'liv',\n",
              " ':)',\n",
              " 'hello',\n",
              " 'need',\n",
              " 'know',\n",
              " 'something',\n",
              " 'u',\n",
              " 'fm',\n",
              " 'twitter',\n",
              " '—',\n",
              " 'sure',\n",
              " 'thing',\n",
              " ':)',\n",
              " 'dm',\n",
              " 'x',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'new',\n",
              " 'follower',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " \"i've\",\n",
              " 'hear',\n",
              " 'four',\n",
              " 'seasons',\n",
              " 'pretty',\n",
              " 'dope',\n",
              " 'penthouse',\n",
              " 'obvs',\n",
              " '#gobigorgohome',\n",
              " 'fun',\n",
              " \"y'all\",\n",
              " ':)',\n",
              " 'yeah',\n",
              " 'suppose',\n",
              " 'lol',\n",
              " 'chat',\n",
              " 'bit',\n",
              " 'x',\n",
              " ':)',\n",
              " 'hello',\n",
              " ':)',\n",
              " 'get',\n",
              " 'youth',\n",
              " 'job',\n",
              " 'opportunities',\n",
              " 'follow',\n",
              " '💅',\n",
              " '🏽',\n",
              " '💋',\n",
              " ':)',\n",
              " 'see',\n",
              " 'year',\n",
              " 'thank',\n",
              " ':-)',\n",
              " 'hope',\n",
              " 'rest',\n",
              " 'night',\n",
              " 'go',\n",
              " 'quickly',\n",
              " '...',\n",
              " 'bed',\n",
              " '...',\n",
              " 'get',\n",
              " 'music',\n",
              " 'fix',\n",
              " 'time',\n",
              " 'dream',\n",
              " ':)',\n",
              " 'spiritual',\n",
              " 'ritual',\n",
              " 'festival',\n",
              " 'népal',\n",
              " 'beginning',\n",
              " 'line-up',\n",
              " ':)',\n",
              " 'leave',\n",
              " 'line-up',\n",
              " 'see',\n",
              " '...',\n",
              " 'hey',\n",
              " 'sarah',\n",
              " 'send',\n",
              " 'us',\n",
              " 'email',\n",
              " 'bitsy.com',\n",
              " \"we'll\",\n",
              " 'help',\n",
              " 'asap',\n",
              " ':)',\n",
              " 'lols',\n",
              " ':d',\n",
              " 'kik',\n",
              " 'hatessuce',\n",
              " '32429',\n",
              " '#kik',\n",
              " '#kikme',\n",
              " '#lgbt',\n",
              " '#tinder',\n",
              " '#nsfw',\n",
              " '#akua',\n",
              " '#cumshot',\n",
              " ':)',\n",
              " 'come',\n",
              " 'house',\n",
              " ':)',\n",
              " '#nsn_supplements',\n",
              " 'effective',\n",
              " 'press',\n",
              " 'release',\n",
              " 'distribution',\n",
              " 'result',\n",
              " ':)',\n",
              " 'link',\n",
              " 'remove',\n",
              " '#pressrelease',\n",
              " '#newsdistribution',\n",
              " 'hi',\n",
              " 'bam',\n",
              " 'follow',\n",
              " 'bestfriend',\n",
              " 'love',\n",
              " 'lot',\n",
              " ':)',\n",
              " 'see',\n",
              " 'warsaw',\n",
              " '<3',\n",
              " 'love',\n",
              " '<3',\n",
              " 'x46',\n",
              " 'everyone',\n",
              " 'watch',\n",
              " 'documentary',\n",
              " 'earthlings',\n",
              " 'youtube',\n",
              " ':-)',\n",
              " 'follow',\n",
              " 'follow',\n",
              " 'u',\n",
              " 'back',\n",
              " ':)',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'support',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'buuuuuuuut',\n",
              " 'oh',\n",
              " 'well',\n",
              " ':-)',\n",
              " 'look',\n",
              " 'forward',\n",
              " 'visit',\n",
              " 'next',\n",
              " 'week',\n",
              " '#letsgetmessy',\n",
              " 'jo',\n",
              " ':-)',\n",
              " 'make',\n",
              " 'u',\n",
              " 'feel',\n",
              " 'good',\n",
              " 'never',\n",
              " 'see',\n",
              " 'anyone',\n",
              " 'kpop',\n",
              " 'flesh',\n",
              " ':d',\n",
              " 'good',\n",
              " 'girl',\n",
              " 'best',\n",
              " 'wish',\n",
              " ':-)',\n",
              " 'great',\n",
              " 'enough',\n",
              " 'reason',\n",
              " 'listen',\n",
              " 'one',\n",
              " 'epic',\n",
              " 'soundtrack',\n",
              " ':d',\n",
              " 'thank',\n",
              " 'shout',\n",
              " 'great',\n",
              " 'friday',\n",
              " ':)',\n",
              " 'add',\n",
              " 'video',\n",
              " 'playlist',\n",
              " 'im',\n",
              " 'back',\n",
              " 'twitch',\n",
              " 'today',\n",
              " 'go',\n",
              " 'league',\n",
              " ':)',\n",
              " '1',\n",
              " '4',\n",
              " 'would',\n",
              " 'love',\n",
              " 'see',\n",
              " 'dear',\n",
              " '#jordan',\n",
              " ':)',\n",
              " 'wait',\n",
              " ':)',\n",
              " 'oh',\n",
              " 'okay',\n",
              " ':d',\n",
              " 'thanks',\n",
              " 'like',\n",
              " 'fake',\n",
              " 'gameplays',\n",
              " ';)',\n",
              " 'haha',\n",
              " 'im',\n",
              " 'kidding',\n",
              " 'im',\n",
              " 'kidding',\n",
              " 'good',\n",
              " 'stuff',\n",
              " ':)',\n",
              " 'yeah',\n",
              " 'exactly',\n",
              " ':)',\n",
              " 'new',\n",
              " 'product',\n",
              " 'line',\n",
              " '#etsy',\n",
              " 'shop',\n",
              " 'check',\n",
              " ':)',\n",
              " '#boxroomcrafts',\n",
              " 'hope',\n",
              " 'vacation',\n",
              " 'go',\n",
              " 'great',\n",
              " ':d',\n",
              " 'rechargeable',\n",
              " 'normally',\n",
              " 'come',\n",
              " 'charger',\n",
              " 'u',\n",
              " 'buy',\n",
              " ':)',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'engage',\n",
              " 'member',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'well',\n",
              " 'asleep',\n",
              " 'one',\n",
              " 'talk',\n",
              " 'sooo',\n",
              " 'someone',\n",
              " 'text',\n",
              " ':)',\n",
              " 'yes',\n",
              " 'suppose',\n",
              " 'bet',\n",
              " \"he'll\",\n",
              " 'blue',\n",
              " 'fit',\n",
              " 'after',\n",
              " 'hear',\n",
              " 'speech',\n",
              " 'today',\n",
              " 'pity',\n",
              " ':-)',\n",
              " 'green',\n",
              " 'garden',\n",
              " 'midnight',\n",
              " 'sun',\n",
              " 'beautiful',\n",
              " 'canal',\n",
              " 'dasvidaniya',\n",
              " 'till',\n",
              " 'next',\n",
              " 'visit',\n",
              " ':)',\n",
              " 'scout',\n",
              " 'sg',\n",
              " 'future',\n",
              " 'wlan',\n",
              " 'pro',\n",
              " 'conference',\n",
              " 'asia',\n",
              " ':)',\n",
              " 'good',\n",
              " 'u',\n",
              " 'good',\n",
              " 'change',\n",
              " 'lollipop',\n",
              " '🍭',\n",
              " ':)',\n",
              " '<3',\n",
              " 'u',\n",
              " 'nez',\n",
              " ':)',\n",
              " '#agnezmo',\n",
              " 'follow',\n",
              " 'please',\n",
              " ':)',\n",
              " 'big',\n",
              " 'dream',\n",
              " ':)',\n",
              " 'oley',\n",
              " ':d',\n",
              " 'mama',\n",
              " 'reason',\n",
              " 'stand',\n",
              " 'strong',\n",
              " ':)',\n",
              " 'follow',\n",
              " 'follow',\n",
              " 'u',\n",
              " 'back',\n",
              " ':)',\n",
              " 'oh',\n",
              " 'god',\n",
              " 'misty',\n",
              " 'baby',\n",
              " 'cute',\n",
              " ':d',\n",
              " 'love',\n",
              " 'blue',\n",
              " ':)',\n",
              " '#flipkartfashionfriday',\n",
              " 'woohoo',\n",
              " \"can't\",\n",
              " 'wait',\n",
              " ':)',\n",
              " 'sign',\n",
              " 'yet',\n",
              " 'still',\n",
              " 'think',\n",
              " 'mka',\n",
              " 'liam',\n",
              " 'access',\n",
              " ':)',\n",
              " 'welcome',\n",
              " ':)',\n",
              " 'stats',\n",
              " 'day',\n",
              " 'arrive',\n",
              " '1',\n",
              " 'new',\n",
              " 'follower',\n",
              " 'unfollowers',\n",
              " ':)',\n",
              " 'via',\n",
              " \"shouldn't\",\n",
              " 'surprise',\n",
              " '...',\n",
              " 'amazing',\n",
              " 'figure',\n",
              " ':)',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'influencers',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " '#happybirthdayemilybett',\n",
              " ':)',\n",
              " 'wishing',\n",
              " 'best',\n",
              " 'beautiful',\n",
              " 'sweet',\n",
              " 'talented',\n",
              " 'amaze',\n",
              " '…',\n",
              " '2',\n",
              " 'plan',\n",
              " 'day',\n",
              " 'drain',\n",
              " 'great',\n",
              " ':)',\n",
              " 'gotta',\n",
              " 'love',\n",
              " 'timezones',\n",
              " ':p',\n",
              " 'parent',\n",
              " 'proud',\n",
              " '—',\n",
              " 'lol',\n",
              " 'not',\n",
              " 'least',\n",
              " ':d',\n",
              " 'maybe',\n",
              " 'sometimes',\n",
              " 'get',\n",
              " 'happy',\n",
              " 'grade',\n",
              " 'al',\n",
              " '...',\n",
              " 'grande',\n",
              " ':)',\n",
              " 'manila_bro',\n",
              " 'choose',\n",
              " 'follow',\n",
              " 'twitter',\n",
              " 'thanks',\n",
              " 'follow',\n",
              " ':)',\n",
              " 'sure',\n",
              " 'let',\n",
              " 'know',\n",
              " 'around',\n",
              " '..',\n",
              " 'side',\n",
              " 'world',\n",
              " '#eh',\n",
              " 'u',\n",
              " 'too',\n",
              " 'take',\n",
              " 'care',\n",
              " ':-)',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'new',\n",
              " 'follower',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'finally',\n",
              " 'fuck',\n",
              " 'weekend',\n",
              " ':)',\n",
              " 'real',\n",
              " ':)',\n",
              " 'thanks',\n",
              " ':d',\n",
              " 'hi',\n",
              " 'bam',\n",
              " 'follow',\n",
              " 'bestfriend',\n",
              " 'love',\n",
              " 'lot',\n",
              " ':)',\n",
              " 'see',\n",
              " 'warsaw',\n",
              " '<3',\n",
              " 'love',\n",
              " '<3',\n",
              " 'x45',\n",
              " 'yes',\n",
              " 'joined',\n",
              " '#hushedcallwithfraydoe',\n",
              " 'call',\n",
              " 'gift',\n",
              " '<3',\n",
              " 'gotta',\n",
              " 'get',\n",
              " ':d',\n",
              " 'yeahhh',\n",
              " ':)',\n",
              " 'dm',\n",
              " 'make',\n",
              " 'night',\n",
              " 'good',\n",
              " ':)',\n",
              " 'join',\n",
              " '#hushedpinwithsammy',\n",
              " 'event',\n",
              " ':d',\n",
              " 'might',\n",
              " 'get',\n",
              " 'text',\n",
              " 'day',\n",
              " 'luv',\n",
              " 'u',\n",
              " 'would',\n",
              " 'really',\n",
              " 'appreciate',\n",
              " 'share',\n",
              " 'video',\n",
              " 'around',\n",
              " ':)',\n",
              " 'hello',\n",
              " ':)',\n",
              " 'get',\n",
              " 'youth',\n",
              " 'job',\n",
              " 'opportunities',\n",
              " 'follow',\n",
              " 'oh',\n",
              " 'wow',\n",
              " 'beautiful',\n",
              " 'tom',\n",
              " ':)',\n",
              " 'love',\n",
              " ':)',\n",
              " 'add',\n",
              " 'video',\n",
              " 'playlist',\n",
              " 'im',\n",
              " 'back',\n",
              " 'twitch',\n",
              " 'today',\n",
              " 'go',\n",
              " 'league',\n",
              " ':)',\n",
              " '1',\n",
              " '3',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'support',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'follow',\n",
              " 'follow',\n",
              " 'u',\n",
              " 'back',\n",
              " ':)',\n",
              " 'gym',\n",
              " 'monday',\n",
              " \"can't\",\n",
              " 'wait',\n",
              " ':)',\n",
              " 'likes',\n",
              " 'hey',\n",
              " \"here's\",\n",
              " 'invite',\n",
              " 'join',\n",
              " 'scope',\n",
              " 'influencer',\n",
              " ':)',\n",
              " 'friend',\n",
              " 'know',\n",
              " ':)',\n",
              " 'wait',\n",
              " 'nude',\n",
              " ':-)',\n",
              " 'go',\n",
              " 'sleep',\n",
              " 'u',\n",
              " ':)',\n",
              " 'stats',\n",
              " 'day',\n",
              " 'arrive',\n",
              " '1',\n",
              " 'new',\n",
              " 'follower',\n",
              " 'unfollowers',\n",
              " ':)',\n",
              " 'via',\n",
              " 'birthday',\n",
              " 'week',\n",
              " 'today',\n",
              " ':d',\n",
              " 'want',\n",
              " 't-shirts',\n",
              " 'cool',\n",
              " ':d',\n",
              " 'haw',\n",
              " 'phela',\n",
              " 'not',\n",
              " 'look',\n",
              " 'like',\n",
              " 'mom',\n",
              " 'obviously',\n",
              " 'look',\n",
              " 'like',\n",
              " ':)',\n",
              " 'prince',\n",
              " 'charm',\n",
              " 'stage',\n",
              " ':)',\n",
              " 'x',\n",
              " 'really',\n",
              " 'good',\n",
              " 'luck',\n",
              " ':)',\n",
              " 'stats',\n",
              " 'day',\n",
              " 'arrive',\n",
              " '1',\n",
              " 'new',\n",
              " 'follower',\n",
              " 'unfollowers',\n",
              " ':)',\n",
              " 'via',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'engage',\n",
              " 'member',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'tyler',\n",
              " 'hipster',\n",
              " 'glass',\n",
              " ':d',\n",
              " 'hey',\n",
              " 'marty',\n",
              " 'glad',\n",
              " 'see',\n",
              " 'twitter',\n",
              " ':)',\n",
              " 'hi',\n",
              " 'join',\n",
              " 'today',\n",
              " ':)',\n",
              " 'thanks',\n",
              " ':-)',\n",
              " 'already',\n",
              " 'afternoon',\n",
              " 'let',\n",
              " 'read',\n",
              " 'al',\n",
              " 'kahfi',\n",
              " 'day',\n",
              " 'finish',\n",
              " ':)',\n",
              " 'ohmyg',\n",
              " 'yaya',\n",
              " 'dub',\n",
              " 'im',\n",
              " 'stalk',\n",
              " 'ig',\n",
              " 'accnt',\n",
              " '..',\n",
              " 'gondooo',\n",
              " 'moo',\n",
              " 'tologooo',\n",
              " ':)',\n",
              " 'haha',\n",
              " \"here's\",\n",
              " 'invite',\n",
              " 'become',\n",
              " 'scope',\n",
              " 'influencer',\n",
              " ':)',\n",
              " 'details',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'influencers',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'thanks',\n",
              " ':)',\n",
              " 'zzz',\n",
              " 'xx',\n",
              " 'physiotherapy',\n",
              " 'friday',\n",
              " 'hashtag',\n",
              " 'today',\n",
              " 'custom',\n",
              " ':-)',\n",
              " 'know',\n",
              " '💪',\n",
              " 'monica',\n",
              " 'miss',\n",
              " 'yeah',\n",
              " 'sound',\n",
              " 'good',\n",
              " ':)',\n",
              " 'good',\n",
              " 'morning',\n",
              " 'beautiful',\n",
              " ':)',\n",
              " \"that's\",\n",
              " 'take',\n",
              " 'hi',\n",
              " 'bam',\n",
              " 'follow',\n",
              " 'bestfriend',\n",
              " 'love',\n",
              " 'lot',\n",
              " ':)',\n",
              " 'see',\n",
              " 'warsaw',\n",
              " '<3',\n",
              " 'love',\n",
              " '<3',\n",
              " 'x43',\n",
              " 'oh',\n",
              " 'yeah',\n",
              " 'definitely',\n",
              " 'go',\n",
              " 'try',\n",
              " 'tonight',\n",
              " ':)',\n",
              " 'take',\n",
              " 'advice',\n",
              " 'too',\n",
              " 'treviso',\n",
              " 'follow',\n",
              " 'follow',\n",
              " 'u',\n",
              " 'back',\n",
              " ':)',\n",
              " 'hi',\n",
              " 'would',\n",
              " 'like',\n",
              " 'concert',\n",
              " 'let',\n",
              " 'know',\n",
              " 'city',\n",
              " 'country',\n",
              " \"i'll\",\n",
              " 'start',\n",
              " 'work',\n",
              " 'thanks',\n",
              " ':)',\n",
              " 'fine',\n",
              " '...',\n",
              " 'gorgeous',\n",
              " 'friday',\n",
              " 'friend',\n",
              " '..',\n",
              " 'xo',\n",
              " '..',\n",
              " ':)',\n",
              " 'oven',\n",
              " 'roast',\n",
              " 'garlic',\n",
              " 'olive',\n",
              " 'oil',\n",
              " 'sun',\n",
              " 'dry',\n",
              " 'tomato',\n",
              " 'dried',\n",
              " 'basil',\n",
              " 'century',\n",
              " 'tuna',\n",
              " ':)',\n",
              " 'right',\n",
              " 'back',\n",
              " 'atchya',\n",
              " 'great',\n",
              " 'day',\n",
              " 'everyone',\n",
              " ':)',\n",
              " 'even',\n",
              " 'follow',\n",
              " 'back',\n",
              " ':-)',\n",
              " 'future',\n",
              " 'almost',\n",
              " ':)',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'new',\n",
              " 'follower',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'chance',\n",
              " ':)',\n",
              " 'cheer',\n",
              " ':)',\n",
              " 'go',\n",
              " 'po',\n",
              " 'ice',\n",
              " 'cream',\n",
              " ':)',\n",
              " 'agree',\n",
              " '100',\n",
              " ':)',\n",
              " 'hehehehe',\n",
              " 'thats',\n",
              " 'point',\n",
              " ':p',\n",
              " 'next',\n",
              " 'time',\n",
              " 'stay',\n",
              " 'home',\n",
              " ':)',\n",
              " 'thanks',\n",
              " ':)',\n",
              " '#followfriday',\n",
              " 'top',\n",
              " 'support',\n",
              " 'community',\n",
              " 'week',\n",
              " ':)',\n",
              " 'soon',\n",
              " 'promise',\n",
              " ':)',\n",
              " 'web',\n",
              " 'whatsapp',\n",
              " 'volta',\n",
              " 'funcionar',\n",
              " 'com',\n",
              " 'iphone',\n",
              " 'jailbroken',\n",
              " ':-)',\n",
              " 'plan',\n",
              " 'watch',\n",
              " 'late',\n",
              " '34',\n",
              " 'min',\n",
              " 'leia',\n",
              " 'appear',\n",
              " 'hologram',\n",
              " 'r2d2',\n",
              " 'w',\n",
              " 'message',\n",
              " 'obi',\n",
              " 'wan',\n",
              " 'sit',\n",
              " 'w',\n",
              " 'luke',\n",
              " ':)',\n",
              " 'inter',\n",
              " '3',\n",
              " 'ucl',\n",
              " 'arsenal',\n",
              " '...',\n",
              " 'small',\n",
              " 'team',\n",
              " 'right',\n",
              " ':)',\n",
              " 'passing',\n",
              " ':)',\n",
              " '🚂',\n",
              " 'dewsbury',\n",
              " 'railway',\n",
              " 'station',\n",
              " 'dew',\n",
              " 'dewsbury',\n",
              " 'west',\n",
              " 'yorkshire',\n",
              " ':)',\n",
              " '430',\n",
              " 'smh',\n",
              " '9:25',\n",
              " ':)',\n",
              " 'live',\n",
              " 'scotland',\n",
              " ':)',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNxAa8Fl_LF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d78ec07c-aab3-4032-fbcd-b0d534262b2c"
      },
      "source": [
        "negative_words"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hopeless',\n",
              " 'tmr',\n",
              " ':(',\n",
              " 'everything',\n",
              " 'kid',\n",
              " 'section',\n",
              " 'ikea',\n",
              " 'cute',\n",
              " 'shame',\n",
              " \"i'm\",\n",
              " 'nearly',\n",
              " '19',\n",
              " '2',\n",
              " 'month',\n",
              " ':(',\n",
              " 'heart',\n",
              " 'slide',\n",
              " 'waste',\n",
              " 'basket',\n",
              " ':(',\n",
              " '“',\n",
              " 'hate',\n",
              " 'japanese',\n",
              " 'call',\n",
              " 'ban',\n",
              " ':(',\n",
              " ':(',\n",
              " '”',\n",
              " 'too',\n",
              " 'dang',\n",
              " 'start',\n",
              " 'next',\n",
              " 'week',\n",
              " 'work',\n",
              " ':(',\n",
              " 'oh',\n",
              " 'god',\n",
              " 'baby',\n",
              " 'face',\n",
              " ':(',\n",
              " 'make',\n",
              " 'smile',\n",
              " ':(',\n",
              " 'work',\n",
              " 'neighbour',\n",
              " 'motor',\n",
              " 'asked',\n",
              " 'say',\n",
              " 'hat',\n",
              " 'update',\n",
              " 'search',\n",
              " ':(',\n",
              " ':(',\n",
              " 'sialan',\n",
              " ':(',\n",
              " 'athabasca',\n",
              " 'glacier',\n",
              " '#1948',\n",
              " ':-(',\n",
              " '#athabasca',\n",
              " '#glacier',\n",
              " '#jasper',\n",
              " '#jaspernationalpark',\n",
              " '#alberta',\n",
              " '#explorealberta',\n",
              " '…',\n",
              " 'really',\n",
              " 'good',\n",
              " 'g',\n",
              " 'idea',\n",
              " \"i'm\",\n",
              " 'never',\n",
              " 'go',\n",
              " 'meet',\n",
              " ':(',\n",
              " 'mare',\n",
              " 'ivan',\n",
              " ':(',\n",
              " 'happy',\n",
              " 'trip',\n",
              " 'keep',\n",
              " 'safe',\n",
              " 'see',\n",
              " 'soon',\n",
              " ':(',\n",
              " \"i'm\",\n",
              " 'tire',\n",
              " 'hahahah',\n",
              " ':(',\n",
              " 'knee',\n",
              " 'replacement',\n",
              " 'get',\n",
              " 'day',\n",
              " ':-(',\n",
              " 'ouch',\n",
              " 'relate',\n",
              " 'sweet',\n",
              " 'n',\n",
              " 'sour',\n",
              " 'kind',\n",
              " 'bi-polar',\n",
              " 'people',\n",
              " 'life',\n",
              " '...',\n",
              " 'cuz',\n",
              " 'life',\n",
              " '...',\n",
              " 'full',\n",
              " '...',\n",
              " ':(',\n",
              " 'pleasse',\n",
              " ':(',\n",
              " 'im',\n",
              " 'not',\n",
              " 'sure',\n",
              " 'tho',\n",
              " ':(',\n",
              " 'feel',\n",
              " 'stupid',\n",
              " \"can't\",\n",
              " 'seem',\n",
              " 'grasp',\n",
              " 'basic',\n",
              " 'digital',\n",
              " 'painting',\n",
              " 'nothing',\n",
              " \"i've\",\n",
              " 'research',\n",
              " 'help',\n",
              " ':(',\n",
              " 'good',\n",
              " 'lord',\n",
              " ':(',\n",
              " 'feel',\n",
              " 'lonely',\n",
              " 'someone',\n",
              " 'talk',\n",
              " 'guy',\n",
              " 'girl',\n",
              " ':(',\n",
              " 'assignment',\n",
              " 'project',\n",
              " ':(',\n",
              " 'really',\n",
              " '😩',\n",
              " 'want',\n",
              " 'play',\n",
              " 'video',\n",
              " 'game',\n",
              " 'watch',\n",
              " 'movie',\n",
              " 'someone',\n",
              " ':(',\n",
              " 'choreographing',\n",
              " 'hard',\n",
              " 'what',\n",
              " 'email',\n",
              " 'link',\n",
              " 'still',\n",
              " 'say',\n",
              " 'long',\n",
              " 'available',\n",
              " ':(',\n",
              " 'cry',\n",
              " 'bc',\n",
              " 'miss',\n",
              " 'mingming',\n",
              " 'much',\n",
              " ':-(',\n",
              " 'sorry',\n",
              " ':(',\n",
              " 'mom',\n",
              " 'far',\n",
              " 'away',\n",
              " ':(',\n",
              " \"we're\",\n",
              " 'truly',\n",
              " 'sorry',\n",
              " ':(',\n",
              " 'safe',\n",
              " 'flight',\n",
              " 'friend',\n",
              " ':(',\n",
              " 'oh',\n",
              " ':(',\n",
              " 'hate',\n",
              " 'happen',\n",
              " 'get',\n",
              " 'sad',\n",
              " 'too',\n",
              " 'oh',\n",
              " 'dog',\n",
              " 'pee',\n",
              " '’',\n",
              " 'bag',\n",
              " ':-(',\n",
              " '’',\n",
              " 'take',\n",
              " '#newwine15',\n",
              " 'doushite',\n",
              " ':(',\n",
              " 'too',\n",
              " 'late',\n",
              " ':(',\n",
              " 'suck',\n",
              " 'much',\n",
              " 'sick',\n",
              " 'plan',\n",
              " 'start',\n",
              " 'work',\n",
              " 'first',\n",
              " 'gundam',\n",
              " 'night',\n",
              " 'nope',\n",
              " ':(',\n",
              " '2',\n",
              " 'dollar',\n",
              " ':(',\n",
              " '😭',\n",
              " '😭',\n",
              " '😭',\n",
              " 'listening',\n",
              " 'back',\n",
              " 'old',\n",
              " 'show',\n",
              " 'know',\n",
              " \"i'm\",\n",
              " 'weird',\n",
              " 'get',\n",
              " 'u',\n",
              " 'leave',\n",
              " 'might',\n",
              " 'give',\n",
              " 'pale',\n",
              " 'imitation',\n",
              " 'after',\n",
              " ':-(',\n",
              " 'go',\n",
              " 'sea',\n",
              " 'massive',\n",
              " 'fucking',\n",
              " 'rash',\n",
              " 'body',\n",
              " 'painful',\n",
              " 'thing',\n",
              " 'ever',\n",
              " 'want',\n",
              " 'go',\n",
              " 'home',\n",
              " ':(',\n",
              " 'hi',\n",
              " 'absent',\n",
              " ':(',\n",
              " 'gran',\n",
              " 'tho',\n",
              " 'know',\n",
              " 'care',\n",
              " 'tell',\n",
              " ':(',\n",
              " 'cute',\n",
              " 'love',\n",
              " 'much',\n",
              " 'wish',\n",
              " 'would',\n",
              " 'sequel',\n",
              " ':(',\n",
              " 'busy',\n",
              " 'sa',\n",
              " 'school',\n",
              " ':(',\n",
              " 'next',\n",
              " 'time',\n",
              " 'love',\n",
              " 'yah',\n",
              " 'xx',\n",
              " 'ouucchhh',\n",
              " 'one',\n",
              " 'wisdom',\n",
              " 'teeth',\n",
              " 'come',\n",
              " ':(',\n",
              " 'frighten',\n",
              " 'case',\n",
              " 'really',\n",
              " 'get',\n",
              " ':(',\n",
              " 'pret',\n",
              " ':(',\n",
              " 'wkwkw',\n",
              " 'verfied',\n",
              " 'active',\n",
              " 'forget',\n",
              " 'follow',\n",
              " 'member',\n",
              " 'thanks',\n",
              " 'join',\n",
              " 'goodbye',\n",
              " '´',\n",
              " 'get',\n",
              " 'chain',\n",
              " 'love',\n",
              " '´',\n",
              " '—',\n",
              " 'sentir-se',\n",
              " 'incompleta',\n",
              " 'okay',\n",
              " '..',\n",
              " '..',\n",
              " ':(',\n",
              " 'go',\n",
              " 'wednesday',\n",
              " ':(',\n",
              " 'marvellous',\n",
              " 'not',\n",
              " 'very',\n",
              " 'thwarting',\n",
              " ':-(',\n",
              " 'awh',\n",
              " \"what's\",\n",
              " 'chance',\n",
              " '😩',\n",
              " 'u',\n",
              " 'zante',\n",
              " 'need',\n",
              " 'something',\n",
              " ':-(',\n",
              " 'x',\n",
              " \"when's\",\n",
              " 'birthday',\n",
              " ':(',\n",
              " 'bad',\n",
              " 'part',\n",
              " 'still',\n",
              " 'feel',\n",
              " 'bad',\n",
              " ':(',\n",
              " 'audraesar',\n",
              " 'sushi',\n",
              " 'pic',\n",
              " 'tl',\n",
              " 'drive',\n",
              " 'craaaazzyy',\n",
              " ':(',\n",
              " 'really',\n",
              " 'want',\n",
              " ':(',\n",
              " 'popped',\n",
              " 'like',\n",
              " 'helium',\n",
              " 'balloon',\n",
              " '..',\n",
              " ':-(',\n",
              " '#climatechange',\n",
              " '#cc',\n",
              " \"california's\",\n",
              " 'powerful',\n",
              " 'influential',\n",
              " 'air',\n",
              " 'pollution',\n",
              " 'watchdog',\n",
              " 'califor',\n",
              " '...',\n",
              " '#uniteblue',\n",
              " '#tcot',\n",
              " ':-(',\n",
              " 'sad',\n",
              " 'elhaida',\n",
              " 'rob',\n",
              " 'jury',\n",
              " ':(',\n",
              " 'come',\n",
              " '10th',\n",
              " 'televoting',\n",
              " '#climatechange',\n",
              " '#cc',\n",
              " 'idaho',\n",
              " 'not',\n",
              " 'restrict',\n",
              " 'fish',\n",
              " 'despite',\n",
              " 'regional',\n",
              " 'drought-linked',\n",
              " 'die-of',\n",
              " '...',\n",
              " '#uniteblue',\n",
              " '#tcot',\n",
              " ':-(',\n",
              " '#climatechange',\n",
              " '#cc',\n",
              " 'abrupt',\n",
              " 'climate',\n",
              " 'change',\n",
              " 'may',\n",
              " 'doom',\n",
              " 'mammoth',\n",
              " 'megafauna',\n",
              " 'sc',\n",
              " '...',\n",
              " '#uniteblue',\n",
              " '#tcot',\n",
              " ':-(',\n",
              " '#climatechange',\n",
              " '#cc',\n",
              " \"australia's\",\n",
              " 'dirtiest',\n",
              " 'power',\n",
              " 'station',\n",
              " 'considers',\n",
              " 'clean',\n",
              " 'energy',\n",
              " 'biomass',\n",
              " '...',\n",
              " '#uniteblue',\n",
              " '#tcot',\n",
              " ':-(',\n",
              " '#climatechange',\n",
              " '#cc',\n",
              " \"ain't\",\n",
              " 'easy',\n",
              " 'green',\n",
              " 'golf',\n",
              " 'course',\n",
              " 'california',\n",
              " 'ulti',\n",
              " '...',\n",
              " '#uniteblue',\n",
              " '#tcot',\n",
              " ':-(',\n",
              " 'well',\n",
              " 'sure',\n",
              " 'work',\n",
              " 'day',\n",
              " 'mine',\n",
              " ':(',\n",
              " 'im',\n",
              " 'gonna',\n",
              " 'miss',\n",
              " 'u',\n",
              " 'sexy',\n",
              " 'prexy',\n",
              " ':(',\n",
              " 'miss',\n",
              " 'kindergarten',\n",
              " 'kid',\n",
              " ':(',\n",
              " 'hungry',\n",
              " ':-(',\n",
              " 'cant',\n",
              " 'find',\n",
              " 'book',\n",
              " 'keep',\n",
              " 'sane',\n",
              " ':(',\n",
              " 'literally',\n",
              " 'three',\n",
              " 'lounge',\n",
              " 'event',\n",
              " ':-(',\n",
              " 'much',\n",
              " 'turn',\n",
              " \"i'm\",\n",
              " 'sad',\n",
              " 'miss',\n",
              " 'boss',\n",
              " ':(',\n",
              " 'love',\n",
              " 'hozier',\n",
              " ':-(',\n",
              " \"that's\",\n",
              " 'true',\n",
              " 'want',\n",
              " 'soooooooner',\n",
              " ':(',\n",
              " 'ahh',\n",
              " 'fam',\n",
              " ':(',\n",
              " '#respectlost',\n",
              " \"i'm\",\n",
              " 'sorry',\n",
              " ':(',\n",
              " 'what',\n",
              " 'hypercholesteloremia',\n",
              " 'ok',\n",
              " 'baby',\n",
              " 'still',\n",
              " 'look',\n",
              " 'tired',\n",
              " ':(',\n",
              " 'someone',\n",
              " 'gift',\n",
              " 'calibraska',\n",
              " ':(',\n",
              " 'massive',\n",
              " 'shame',\n",
              " 'would',\n",
              " 'actually',\n",
              " 'genuine',\n",
              " 'contender',\n",
              " \"won't\",\n",
              " 'happen',\n",
              " ':(',\n",
              " 'head',\n",
              " 'always',\n",
              " 'hurt',\n",
              " 'stay',\n",
              " 'late',\n",
              " 'lmao',\n",
              " ':(',\n",
              " 'u',\n",
              " 'old',\n",
              " ':(',\n",
              " 'backed',\n",
              " ':(',\n",
              " 'u',\n",
              " 'sound',\n",
              " 'upset',\n",
              " ':(',\n",
              " 'much',\n",
              " 'miss',\n",
              " ':-(',\n",
              " 'miss',\n",
              " 'infinite',\n",
              " ':-(',\n",
              " 'aos',\n",
              " 'want',\n",
              " 'stick',\n",
              " '8th',\n",
              " 'either',\n",
              " ':(',\n",
              " 'too',\n",
              " 'much',\n",
              " 'serious',\n",
              " 'yun',\n",
              " 'eh',\n",
              " ':(',\n",
              " 'room',\n",
              " 'way',\n",
              " 'too',\n",
              " 'hot',\n",
              " ':-(',\n",
              " 'still',\n",
              " 'havent',\n",
              " 'find',\n",
              " 'handsome',\n",
              " 'jack',\n",
              " 'draw',\n",
              " ':(',\n",
              " 'shit',\n",
              " ':(',\n",
              " 'cut',\n",
              " 'encore',\n",
              " ':(',\n",
              " '#bad4thwin',\n",
              " 'wish',\n",
              " 'baymax',\n",
              " ':(',\n",
              " 'sick',\n",
              " ':(',\n",
              " 'french',\n",
              " 'mixer',\n",
              " 'miss',\n",
              " 'much',\n",
              " ':(',\n",
              " '💜',\n",
              " 'wft',\n",
              " '..',\n",
              " \"can't\",\n",
              " 'watch',\n",
              " 'awesome',\n",
              " 'replay',\n",
              " ':-(',\n",
              " 'what',\n",
              " 'happen',\n",
              " ':(',\n",
              " 'party',\n",
              " 'promotion',\n",
              " ':(',\n",
              " 'music',\n",
              " 'bank',\n",
              " 'encore',\n",
              " 'always',\n",
              " 'short',\n",
              " ':(',\n",
              " 'baby',\n",
              " 'boy',\n",
              " ':(',\n",
              " 'order',\n",
              " 'receive',\n",
              " 'hub',\n",
              " 'near',\n",
              " 'look',\n",
              " 'like',\n",
              " 'deliver',\n",
              " 'today',\n",
              " ':(',\n",
              " '1/2',\n",
              " 'mum',\n",
              " 'playing',\n",
              " 'music',\n",
              " 'loud',\n",
              " ':(',\n",
              " 'finale',\n",
              " 'parasyte',\n",
              " 'fuck',\n",
              " 'feeling',\n",
              " 'alllllll',\n",
              " 'way',\n",
              " ':(',\n",
              " 'wish',\n",
              " ':(',\n",
              " '#zayniscomingbackonjuly26',\n",
              " 'good',\n",
              " 'bye',\n",
              " 'party',\n",
              " 'era',\n",
              " ':(',\n",
              " 'too',\n",
              " '。',\n",
              " '。',\n",
              " '。',\n",
              " ':(',\n",
              " '´',\n",
              " 'ω',\n",
              " '」',\n",
              " '∠',\n",
              " '):',\n",
              " 'nathann',\n",
              " '💕',\n",
              " 'never',\n",
              " 'get',\n",
              " 'chance',\n",
              " 'take',\n",
              " 'pic',\n",
              " ':(',\n",
              " 'get',\n",
              " 'hug',\n",
              " 'tho',\n",
              " 'pic',\n",
              " 'next',\n",
              " 'time',\n",
              " 'see',\n",
              " '😊',\n",
              " 'girl',\n",
              " 'come',\n",
              " 'like',\n",
              " 'beautiful',\n",
              " ':-(',\n",
              " 'want',\n",
              " 'dieididieieiei',\n",
              " 'cute',\n",
              " 'party',\n",
              " 'goodbye',\n",
              " 'stage',\n",
              " ':(',\n",
              " 'mean',\n",
              " 'hello',\n",
              " 'lion',\n",
              " 'heart',\n",
              " 'think',\n",
              " 'tell',\n",
              " 'screw',\n",
              " \"i'm\",\n",
              " 'gonna',\n",
              " 'netflix',\n",
              " 'chill',\n",
              " 'u',\n",
              " 'dis',\n",
              " 'ervin',\n",
              " ':(',\n",
              " 'ohh',\n",
              " ':(',\n",
              " 'yeah',\n",
              " 'hope',\n",
              " 'come',\n",
              " 'back',\n",
              " 'soon',\n",
              " 'too',\n",
              " \"i've\",\n",
              " 'accept',\n",
              " 'offer',\n",
              " '..',\n",
              " 'im',\n",
              " 'desperate',\n",
              " 'take',\n",
              " 'year',\n",
              " ':(',\n",
              " 'come',\n",
              " 'back',\n",
              " ':(',\n",
              " 'snapchat',\n",
              " 'amargolonnard',\n",
              " '#snapchat',\n",
              " '#snapchat',\n",
              " '#kikhorny',\n",
              " '#snapme',\n",
              " '#tagsforlikes',\n",
              " '#batalladelosgallos',\n",
              " '#webcamsex',\n",
              " ':(',\n",
              " 'ugh',\n",
              " 'cant',\n",
              " 'stream',\n",
              " 'tmr',\n",
              " 'duty',\n",
              " ':(',\n",
              " \"u've\",\n",
              " 'go',\n",
              " 'too',\n",
              " 'far',\n",
              " ':(',\n",
              " 'what',\n",
              " 'alien',\n",
              " 'thing',\n",
              " ':(',\n",
              " 'aww',\n",
              " 'really',\n",
              " 'wanna',\n",
              " 'get',\n",
              " 'hope',\n",
              " 'wish',\n",
              " 'would',\n",
              " ':(',\n",
              " 'sorka',\n",
              " ':(',\n",
              " 'know',\n",
              " 'u',\n",
              " 'would',\n",
              " 'sad',\n",
              " ':(',\n",
              " 'funeral',\n",
              " 'gonna',\n",
              " 'text',\n",
              " 'u',\n",
              " 'phone',\n",
              " 'sunny',\n",
              " ':(',\n",
              " 'feel',\n",
              " 'bad',\n",
              " '...',\n",
              " 'nonexistent',\n",
              " 'wowza',\n",
              " ':(',\n",
              " 'back',\n",
              " 'fah',\n",
              " 'mine',\n",
              " ':-(',\n",
              " 'taylor',\n",
              " 'crop',\n",
              " ':(',\n",
              " 'boo',\n",
              " 'count',\n",
              " ':-(',\n",
              " 'new',\n",
              " 'guitar',\n",
              " ':(',\n",
              " 'get',\n",
              " 'jonghyun',\n",
              " 'hyung',\n",
              " ':(',\n",
              " 'please',\n",
              " 'predict',\n",
              " ':(',\n",
              " 'hope',\n",
              " 'sj',\n",
              " 'nominate',\n",
              " 'soon',\n",
              " 'sj',\n",
              " 'vs',\n",
              " 'infinite',\n",
              " 'pls',\n",
              " ':(',\n",
              " 'yeah',\n",
              " 'dude',\n",
              " 'keep',\n",
              " 'calm',\n",
              " 'brace',\n",
              " ':-(',\n",
              " 'sir',\n",
              " 'plus',\n",
              " '4',\n",
              " 'please',\n",
              " ':(',\n",
              " 'looked',\n",
              " 'shock',\n",
              " 'omgggg',\n",
              " ':(',\n",
              " 'yall',\n",
              " 'deserve',\n",
              " ':(',\n",
              " 'whenever',\n",
              " 'would',\n",
              " 'spend',\n",
              " 'night',\n",
              " 'would',\n",
              " 'smoke',\n",
              " 'watch',\n",
              " 'movie',\n",
              " 'would',\n",
              " 'always',\n",
              " 'end',\n",
              " 'fall',\n",
              " 'asleep',\n",
              " ':(',\n",
              " '1',\n",
              " 'point',\n",
              " 'close',\n",
              " 'grand',\n",
              " 'final',\n",
              " ':(',\n",
              " 'whyyy',\n",
              " ':(',\n",
              " \"that's\",\n",
              " 'long',\n",
              " 'time',\n",
              " 'oh',\n",
              " 'must',\n",
              " 'annoying',\n",
              " 'evan',\n",
              " ':(',\n",
              " 'text',\n",
              " 'give',\n",
              " 'option',\n",
              " 'opt',\n",
              " \"who's\",\n",
              " 'giveaway',\n",
              " 'muster',\n",
              " 'good',\n",
              " ':(',\n",
              " 'merchs',\n",
              " 'ah',\n",
              " 'too',\n",
              " 'bad',\n",
              " ':(',\n",
              " 'people',\n",
              " 'look',\n",
              " 'funny',\n",
              " \"i'm\",\n",
              " 'drink',\n",
              " 'savanna',\n",
              " 'straw',\n",
              " ':(',\n",
              " 'good',\n",
              " 'bye',\n",
              " 'stage',\n",
              " ':(',\n",
              " 'ignore',\n",
              " 'yester',\n",
              " 'afternoon',\n",
              " ':(',\n",
              " \"can't\",\n",
              " 'sleep',\n",
              " ':(',\n",
              " 'someone',\n",
              " 'talk',\n",
              " 'yeah',\n",
              " ':(',\n",
              " 'yes',\n",
              " 'sadly',\n",
              " ':(',\n",
              " 'very',\n",
              " ':(',\n",
              " 'whens',\n",
              " 'album',\n",
              " 'come',\n",
              " ':(',\n",
              " 'last',\n",
              " 'chocolate',\n",
              " 'consume',\n",
              " ':(',\n",
              " 'ugh',\n",
              " 'werk',\n",
              " 'morning',\n",
              " 'too',\n",
              " ':-(',\n",
              " 'foreals',\n",
              " ':(',\n",
              " 'wesen',\n",
              " 'uwesiti',\n",
              " 'mj',\n",
              " '😂',\n",
              " '😩',\n",
              " 'catch',\n",
              " 'online',\n",
              " 'long',\n",
              " 'enough',\n",
              " 'haha',\n",
              " \"he's\",\n",
              " 'very',\n",
              " 'annoy',\n",
              " ':(',\n",
              " 'bosen',\n",
              " ':(',\n",
              " 'dying',\n",
              " 'egg',\n",
              " 'benny',\n",
              " ':(',\n",
              " 'sometimes',\n",
              " 'hate',\n",
              " ':(',\n",
              " 'baby',\n",
              " 'followback',\n",
              " 'huhu',\n",
              " ':(',\n",
              " 'one',\n",
              " 'cute',\n",
              " 'u',\n",
              " ':(',\n",
              " 'gonna',\n",
              " 'watch',\n",
              " 'shit',\n",
              " 'understand',\n",
              " 'badly',\n",
              " 'scar',\n",
              " '>:(',\n",
              " 'miss',\n",
              " 'al',\n",
              " 'katie',\n",
              " 'zaz',\n",
              " 'amy',\n",
              " 'lot',\n",
              " ':(',\n",
              " 'like',\n",
              " 'video',\n",
              " 'subs',\n",
              " 'plz',\n",
              " ':(',\n",
              " 'u',\n",
              " 'make',\n",
              " 'diary',\n",
              " 'please',\n",
              " ':(',\n",
              " 'sorry',\n",
              " ':(',\n",
              " ':-(',\n",
              " 'read',\n",
              " 'rehash',\n",
              " 'website',\n",
              " 'mushroom',\n",
              " 'good',\n",
              " 'piece',\n",
              " 'may',\n",
              " 'exception',\n",
              " 'reach',\n",
              " 'one',\n",
              " 'anyway',\n",
              " 'yes',\n",
              " 'vicky',\n",
              " 'omg',\n",
              " ':-(',\n",
              " 'wtf',\n",
              " 'like',\n",
              " 'lips',\n",
              " 'still',\n",
              " 'virgin',\n",
              " 'omg',\n",
              " 'mom',\n",
              " 'get',\n",
              " 'life',\n",
              " 'youre',\n",
              " '45',\n",
              " 'hahah',\n",
              " 'ninasty',\n",
              " 'tsktsk',\n",
              " 'oppa',\n",
              " 'wont',\n",
              " 'like',\n",
              " 'u',\n",
              " 'call',\n",
              " 'dick',\n",
              " 'kawaii',\n",
              " 'manly',\n",
              " '>:(',\n",
              " 'lonely',\n",
              " ':(',\n",
              " \"can't\",\n",
              " 'go',\n",
              " 'back',\n",
              " 'sleep',\n",
              " ':(',\n",
              " 'wanna',\n",
              " 'go',\n",
              " 'xbox',\n",
              " 'netflix',\n",
              " 'already',\n",
              " 'too',\n",
              " 'comfy',\n",
              " 'bed',\n",
              " ':(',\n",
              " 'miss',\n",
              " 'youu',\n",
              " ':(',\n",
              " 'sigh',\n",
              " 'feel',\n",
              " 'bad',\n",
              " 'girl',\n",
              " ':(',\n",
              " 'lol',\n",
              " 'love',\n",
              " 'sweet',\n",
              " 'potato',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-k_oAB__LF6",
        "colab_type": "text"
      },
      "source": [
        "which are the most common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8lsFXax_LF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import FreqDist"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwsMtTxj_LF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71c5a867-f29d-4d63-a119-90672de9ef45"
      },
      "source": [
        "freq_dist_pos = FreqDist(positive_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftu7-yLo_LF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1778341a-f16e-4192-a166-99de7e356d98"
      },
      "source": [
        "freq_dist_pos = FreqDist(negative_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(':(', 4585), (':-(', 501), (\"i'm\", 343), ('...', 332), ('get', 325), ('not', 300), ('miss', 291), ('go', 275), ('please', 275), ('want', 246)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr7ofYX__LGA",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Data for the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NomKLseG_LGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token,True] for token in tweet_tokens)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBKFL9Yq_LGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEvlQu4_LGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTii_EMK_LGH",
        "colab_type": "text"
      },
      "source": [
        " prepare the data for training the NaiveBayesClassifier class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XvhBHNS_LGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVD-_XVb_LGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VhD9p0A_LGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#positive_dataset"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gikbOvK0_LGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PLt1pYU_LGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#negative_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfjZa4Jm_LGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = positive_dataset + negative_dataset"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVtROtAB_LGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(dataset)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNEU3HOQ_LGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QZd8WQH_LGY",
        "colab_type": "text"
      },
      "source": [
        "# Building and Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCQWZuxd_LGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htCnsYOP_LGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = NaiveBayesClassifier.train(train_data)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih1_065u_LGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "37c36d98-e8b4-4a3b-d053-d36e0bbbf842"
      },
      "source": [
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.9953333333333333\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2047.3 : 1.0\n",
            "                      :) = True           Positi : Negati =   1671.5 : 1.0\n",
            "                  arrive = True           Positi : Negati =     33.4 : 1.0\n",
            "                     sad = True           Negati : Positi =     33.4 : 1.0\n",
            "                follower = True           Positi : Negati =     22.9 : 1.0\n",
            "              bestfriend = True           Positi : Negati =     19.9 : 1.0\n",
            "                     bam = True           Positi : Negati =     18.6 : 1.0\n",
            "                     x15 = True           Negati : Positi =     17.4 : 1.0\n",
            "                    cool = True           Positi : Negati =     17.2 : 1.0\n",
            "                     ugh = True           Negati : Positi =     16.1 : 1.0\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epy7hb4Y_LGd",
        "colab_type": "text"
      },
      "source": [
        "# Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQyu2w5G_LGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVhU_0PJ_LGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_tweet = \"This is worst movie\""
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcjqq2mN_LGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b64c6b3c-b87a-417f-c180-a4a567457fb5"
      },
      "source": [
        "custom_tokens = word_tokenize(custom_tweet)\n",
        "custom_tokens"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'worst', 'movie']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26ARWDkq_LGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_predict = []\n",
        "for word, tag in pos_tag(custom_tokens):\n",
        "    word=word.lower()\n",
        "    if tag.startswith('NN'):\n",
        "        pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "        pos = 'v'\n",
        "    else:\n",
        "        pos = 'a'\n",
        "    lemmatized_predict.append(lemmatizer.lemmatize(word, pos))\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cPEMIPh_LGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40859c54-988c-4393-e530-f9864ca44771"
      },
      "source": [
        "lemmatized_predict"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'be', 'bad', 'movie']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9nrKVGa_LGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28c9bd52-7dd5-41ce-b18a-a8453f491ab0"
      },
      "source": [
        "print(classifier.classify(dict([token, True] for token in lemmatized_predict)))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}