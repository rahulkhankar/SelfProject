{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHH/cXHQljRZdAnSymteh8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulkhankar/SelfProject/blob/master/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sem3BVAOQkdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "59dacf7c-bb71-442c-eb8d-34516996080e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fskg0QhoTcYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = r'/gdrive/My Drive/Colab Notebooks/Dataset/Next text/chamber_of_secrets.txt'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1NMtXGsUbos",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize and Clean Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzKNDdlzRaHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHGcq8YRUfXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword=['a','an','the','.']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gwKsBIgUhLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def separate_punc(doc_text):\n",
        "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n\"-#$%&()--*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ' if token.text not in stopword]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFVcL_rUior",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = open(r'/gdrive/My Drive/Colab Notebooks/Dataset/Next text/chamber_of_secrets.txt',encoding='utf-8')\n",
        "d=x.read()\n",
        "tokens = separate_punc(d)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCSGfZFPUmnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50c7c2c4-8610-4179-df18-2f0e19f7e99d"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not',\n",
              " 'for',\n",
              " 'first',\n",
              " 'time',\n",
              " 'argument',\n",
              " 'had',\n",
              " 'broken',\n",
              " 'out',\n",
              " 'over',\n",
              " 'breakfast',\n",
              " 'at',\n",
              " 'number',\n",
              " 'four',\n",
              " 'privet',\n",
              " 'drive',\n",
              " 'mr.',\n",
              " 'vernon',\n",
              " 'dursley',\n",
              " 'had',\n",
              " 'been',\n",
              " 'woken',\n",
              " 'in',\n",
              " 'early',\n",
              " 'hours',\n",
              " 'of',\n",
              " 'morning',\n",
              " 'by',\n",
              " 'loud',\n",
              " 'hooting',\n",
              " 'noise',\n",
              " 'from',\n",
              " 'his',\n",
              " 'nephew',\n",
              " 'harry',\n",
              " '’s',\n",
              " 'room',\n",
              " '“',\n",
              " 'third',\n",
              " 'time',\n",
              " 'this',\n",
              " 'week',\n",
              " '!',\n",
              " '”',\n",
              " 'he',\n",
              " 'roared',\n",
              " 'across',\n",
              " 'table',\n",
              " '“',\n",
              " 'if',\n",
              " 'you',\n",
              " 'can',\n",
              " 'not',\n",
              " 'control',\n",
              " 'that',\n",
              " 'owl',\n",
              " 'it',\n",
              " 'will',\n",
              " 'have',\n",
              " 'to',\n",
              " 'go',\n",
              " '!',\n",
              " '”',\n",
              " 'harry',\n",
              " 'tried',\n",
              " 'yet',\n",
              " 'again',\n",
              " 'to',\n",
              " 'explain',\n",
              " '“',\n",
              " 'she',\n",
              " 'is',\n",
              " 'bored',\n",
              " '”',\n",
              " 'he',\n",
              " 'said',\n",
              " '“',\n",
              " 'she',\n",
              " 'is',\n",
              " 'used',\n",
              " 'to',\n",
              " 'flying',\n",
              " 'around',\n",
              " 'outside',\n",
              " 'if',\n",
              " 'i',\n",
              " 'could',\n",
              " 'just',\n",
              " 'let',\n",
              " 'her',\n",
              " 'out',\n",
              " 'at',\n",
              " 'night',\n",
              " '”',\n",
              " '“',\n",
              " 'do',\n",
              " 'i',\n",
              " 'look',\n",
              " 'stupid',\n",
              " '”',\n",
              " 'snarled',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'bit',\n",
              " 'of',\n",
              " 'fried',\n",
              " 'egg',\n",
              " 'dangling',\n",
              " 'from',\n",
              " 'his',\n",
              " 'bushy',\n",
              " 'mustache',\n",
              " '“',\n",
              " 'i',\n",
              " 'know',\n",
              " 'what',\n",
              " 'will',\n",
              " 'happen',\n",
              " 'if',\n",
              " 'that',\n",
              " 'owl',\n",
              " 'is',\n",
              " 'let',\n",
              " 'out',\n",
              " '”',\n",
              " 'he',\n",
              " 'exchanged',\n",
              " 'dark',\n",
              " 'looks',\n",
              " 'with',\n",
              " 'his',\n",
              " 'wife',\n",
              " 'petunia',\n",
              " 'harry',\n",
              " 'tried',\n",
              " 'to',\n",
              " 'argue',\n",
              " 'back',\n",
              " 'but',\n",
              " 'his',\n",
              " 'words',\n",
              " 'were',\n",
              " 'drowned',\n",
              " 'by',\n",
              " 'long',\n",
              " 'loud',\n",
              " 'belch',\n",
              " 'from',\n",
              " 'dursleys',\n",
              " '’',\n",
              " 'son',\n",
              " 'dudley',\n",
              " '“',\n",
              " 'i',\n",
              " 'want',\n",
              " 'more',\n",
              " 'bacon',\n",
              " '”',\n",
              " '“',\n",
              " 'there',\n",
              " 'is',\n",
              " 'more',\n",
              " 'in',\n",
              " 'frying',\n",
              " 'pan',\n",
              " 'sweetums',\n",
              " '”',\n",
              " 'said',\n",
              " 'aunt',\n",
              " 'petunia',\n",
              " 'turning',\n",
              " 'misty',\n",
              " 'eyes',\n",
              " 'on',\n",
              " 'her',\n",
              " 'massive',\n",
              " 'son',\n",
              " '“',\n",
              " 'we',\n",
              " 'must',\n",
              " 'build',\n",
              " 'you',\n",
              " 'up',\n",
              " 'while',\n",
              " 'we',\n",
              " 'have',\n",
              " 'got',\n",
              " 'chance',\n",
              " 'i',\n",
              " 'donot',\n",
              " 'like',\n",
              " 'sound',\n",
              " 'of',\n",
              " 'that',\n",
              " 'school',\n",
              " 'food',\n",
              " '”',\n",
              " '“',\n",
              " 'nonsense',\n",
              " 'petunia',\n",
              " 'i',\n",
              " 'never',\n",
              " 'went',\n",
              " 'hungry',\n",
              " 'when',\n",
              " 'i',\n",
              " 'was',\n",
              " 'at',\n",
              " 'smeltings',\n",
              " '”',\n",
              " 'said',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'heartily',\n",
              " '“',\n",
              " 'dudley',\n",
              " 'gets',\n",
              " 'enough',\n",
              " 'donot',\n",
              " 'you',\n",
              " 'son',\n",
              " '”',\n",
              " 'dudley',\n",
              " 'who',\n",
              " 'was',\n",
              " 'so',\n",
              " 'large',\n",
              " 'his',\n",
              " 'bottom',\n",
              " 'drooped',\n",
              " 'over',\n",
              " 'either',\n",
              " 'side',\n",
              " 'of',\n",
              " 'kitchen',\n",
              " 'chair',\n",
              " 'grinned',\n",
              " 'and',\n",
              " 'turned',\n",
              " 'to',\n",
              " 'harry',\n",
              " '“',\n",
              " 'pass',\n",
              " 'frying',\n",
              " 'pan',\n",
              " '”',\n",
              " '“',\n",
              " 'you',\n",
              " 'have',\n",
              " 'forgotten',\n",
              " 'magic',\n",
              " 'word',\n",
              " '”',\n",
              " 'said',\n",
              " 'harry',\n",
              " 'irritably',\n",
              " 'the',\n",
              " 'effect',\n",
              " 'of',\n",
              " 'this',\n",
              " 'simple',\n",
              " 'sentence',\n",
              " 'on',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'family',\n",
              " 'was',\n",
              " 'incredible',\n",
              " 'dudley',\n",
              " 'gasped',\n",
              " 'and',\n",
              " 'fell',\n",
              " 'off',\n",
              " 'his',\n",
              " 'chair',\n",
              " 'with',\n",
              " 'crash',\n",
              " 'that',\n",
              " 'shook',\n",
              " 'whole',\n",
              " 'kitchen',\n",
              " 'mrs.',\n",
              " 'dursley',\n",
              " 'gave',\n",
              " 'small',\n",
              " 'scream',\n",
              " 'and',\n",
              " 'clapped',\n",
              " 'her',\n",
              " 'hands',\n",
              " 'to',\n",
              " 'her',\n",
              " 'mouth',\n",
              " 'mr.',\n",
              " 'dursley',\n",
              " 'jumped',\n",
              " 'to',\n",
              " 'his',\n",
              " 'feet',\n",
              " 'veins',\n",
              " 'throbbing',\n",
              " 'in',\n",
              " 'his',\n",
              " 'temples',\n",
              " '“',\n",
              " 'i',\n",
              " 'meant',\n",
              " '‘',\n",
              " 'please',\n",
              " '’',\n",
              " '!',\n",
              " '”',\n",
              " 'said',\n",
              " 'harry',\n",
              " 'quickly',\n",
              " '“',\n",
              " 'i',\n",
              " 'didnot',\n",
              " 'mean',\n",
              " '”',\n",
              " '“',\n",
              " 'what',\n",
              " 'have',\n",
              " 'i',\n",
              " 'told',\n",
              " 'you',\n",
              " '”',\n",
              " 'thundered',\n",
              " 'his',\n",
              " 'uncle',\n",
              " 'spraying',\n",
              " 'spit',\n",
              " 'over',\n",
              " 'table',\n",
              " '“',\n",
              " 'about',\n",
              " 'saying',\n",
              " 'the',\n",
              " '‘',\n",
              " 'm',\n",
              " '’',\n",
              " 'word',\n",
              " 'in',\n",
              " 'our',\n",
              " 'house',\n",
              " '”',\n",
              " '“',\n",
              " 'but',\n",
              " 'i',\n",
              " '—',\n",
              " '”',\n",
              " '“',\n",
              " 'how',\n",
              " 'dare',\n",
              " 'you',\n",
              " 'threaten',\n",
              " 'dudley',\n",
              " '!',\n",
              " '”',\n",
              " 'roared',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'pounding',\n",
              " 'table',\n",
              " 'with',\n",
              " 'his',\n",
              " 'fist',\n",
              " '“',\n",
              " 'i',\n",
              " 'just',\n",
              " '”',\n",
              " '“',\n",
              " 'i',\n",
              " 'warned',\n",
              " 'you',\n",
              " '!',\n",
              " 'i',\n",
              " 'will',\n",
              " 'not',\n",
              " 'tolerate',\n",
              " 'mention',\n",
              " 'of',\n",
              " 'your',\n",
              " 'abnormality',\n",
              " 'under',\n",
              " 'this',\n",
              " 'roof',\n",
              " '!',\n",
              " '”',\n",
              " 'harry',\n",
              " 'stared',\n",
              " 'from',\n",
              " 'his',\n",
              " 'purple',\n",
              " 'faced',\n",
              " 'uncle',\n",
              " 'to',\n",
              " 'his',\n",
              " 'pale',\n",
              " 'aunt',\n",
              " 'who',\n",
              " 'was',\n",
              " 'trying',\n",
              " 'to',\n",
              " 'heave',\n",
              " 'dudley',\n",
              " 'to',\n",
              " 'his',\n",
              " 'feet',\n",
              " '“',\n",
              " 'all',\n",
              " 'right',\n",
              " '”',\n",
              " 'said',\n",
              " 'harry',\n",
              " '“',\n",
              " 'all',\n",
              " 'right',\n",
              " '”',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'sat',\n",
              " 'back',\n",
              " 'down',\n",
              " 'breathing',\n",
              " 'like',\n",
              " 'winded',\n",
              " 'rhinoceros',\n",
              " 'and',\n",
              " 'watching',\n",
              " 'harry',\n",
              " 'closely',\n",
              " 'out',\n",
              " 'of',\n",
              " 'corners',\n",
              " 'of',\n",
              " 'his',\n",
              " 'small',\n",
              " 'sharp',\n",
              " 'eyes',\n",
              " 'ever',\n",
              " 'since',\n",
              " 'harry',\n",
              " 'had',\n",
              " 'come',\n",
              " 'home',\n",
              " 'for',\n",
              " 'summer',\n",
              " 'holidays',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'had',\n",
              " 'been',\n",
              " 'treating',\n",
              " 'him',\n",
              " 'like',\n",
              " 'bomb',\n",
              " 'that',\n",
              " 'might',\n",
              " 'go',\n",
              " 'off',\n",
              " 'at',\n",
              " 'any',\n",
              " 'moment',\n",
              " 'because',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'was',\n",
              " 'not',\n",
              " 'normal',\n",
              " 'boy',\n",
              " 'as',\n",
              " 'matter',\n",
              " 'of',\n",
              " 'fact',\n",
              " 'he',\n",
              " 'was',\n",
              " 'as',\n",
              " 'not',\n",
              " 'normal',\n",
              " 'as',\n",
              " 'it',\n",
              " 'is',\n",
              " 'possible',\n",
              " 'to',\n",
              " 'be',\n",
              " 'harry',\n",
              " 'potter',\n",
              " 'was',\n",
              " 'wizard',\n",
              " '—',\n",
              " 'wizard',\n",
              " 'fresh',\n",
              " 'from',\n",
              " 'his',\n",
              " 'first',\n",
              " 'year',\n",
              " 'at',\n",
              " 'hogwarts',\n",
              " 'school',\n",
              " 'of',\n",
              " 'witchcraft',\n",
              " 'and',\n",
              " 'wizardry',\n",
              " 'and',\n",
              " 'if',\n",
              " 'dursleys',\n",
              " 'were',\n",
              " 'unhappy',\n",
              " 'to',\n",
              " 'have',\n",
              " 'him',\n",
              " 'back',\n",
              " 'for',\n",
              " 'holidays',\n",
              " 'it',\n",
              " 'was',\n",
              " 'nothing',\n",
              " 'to',\n",
              " 'how',\n",
              " 'harry',\n",
              " 'felt',\n",
              " 'he',\n",
              " 'missed',\n",
              " 'hogwarts',\n",
              " 'so',\n",
              " 'much',\n",
              " 'it',\n",
              " 'was',\n",
              " 'like',\n",
              " 'having',\n",
              " 'constant',\n",
              " 'stomachache',\n",
              " 'he',\n",
              " 'missed',\n",
              " 'castle',\n",
              " 'with',\n",
              " 'its',\n",
              " 'secret',\n",
              " 'passageways',\n",
              " 'and',\n",
              " 'ghosts',\n",
              " 'his',\n",
              " 'classes',\n",
              " 'though',\n",
              " 'perhaps',\n",
              " 'not',\n",
              " 'snape',\n",
              " 'potions',\n",
              " 'master',\n",
              " 'mail',\n",
              " 'arriving',\n",
              " 'by',\n",
              " 'owl',\n",
              " 'eating',\n",
              " 'banquets',\n",
              " 'in',\n",
              " 'great',\n",
              " 'hall',\n",
              " 'sleeping',\n",
              " 'in',\n",
              " 'his',\n",
              " 'four',\n",
              " 'poster',\n",
              " 'bed',\n",
              " 'in',\n",
              " 'tower',\n",
              " 'dormitory',\n",
              " 'visiting',\n",
              " 'gamekeeper',\n",
              " 'hagrid',\n",
              " 'in',\n",
              " 'his',\n",
              " 'cabin',\n",
              " 'next',\n",
              " 'to',\n",
              " 'forbidden',\n",
              " 'forest',\n",
              " 'in',\n",
              " 'grounds',\n",
              " 'and',\n",
              " 'especially',\n",
              " 'quidditch',\n",
              " 'most',\n",
              " 'popular',\n",
              " 'sport',\n",
              " 'in',\n",
              " 'wizarding',\n",
              " 'world',\n",
              " 'six',\n",
              " 'tall',\n",
              " 'goal',\n",
              " 'posts',\n",
              " 'four',\n",
              " 'flying',\n",
              " 'balls',\n",
              " 'and',\n",
              " 'fourteen',\n",
              " 'players',\n",
              " 'on',\n",
              " 'broomsticks',\n",
              " 'all',\n",
              " 'harry',\n",
              " '’s',\n",
              " 'spellbooks',\n",
              " 'his',\n",
              " 'wand',\n",
              " 'robes',\n",
              " 'cauldron',\n",
              " 'and',\n",
              " 'top',\n",
              " 'ofthe',\n",
              " 'line',\n",
              " 'nimbus',\n",
              " 'two',\n",
              " 'thousand',\n",
              " 'broomstick',\n",
              " 'had',\n",
              " 'been',\n",
              " 'locked',\n",
              " 'in',\n",
              " 'cupboard',\n",
              " 'under',\n",
              " 'stairs',\n",
              " 'by',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'instant',\n",
              " 'harry',\n",
              " 'had',\n",
              " 'come',\n",
              " 'home',\n",
              " 'what',\n",
              " 'did',\n",
              " 'dursleys',\n",
              " 'care',\n",
              " 'if',\n",
              " 'harry',\n",
              " 'lost',\n",
              " 'his',\n",
              " 'place',\n",
              " 'on',\n",
              " 'house',\n",
              " 'quidditch',\n",
              " 'team',\n",
              " 'because',\n",
              " 'he',\n",
              " 'hadnot',\n",
              " 'practiced',\n",
              " 'all',\n",
              " 'summer',\n",
              " 'what',\n",
              " 'was',\n",
              " 'it',\n",
              " 'to',\n",
              " 'dursleys',\n",
              " 'if',\n",
              " 'harry',\n",
              " 'went',\n",
              " 'back',\n",
              " 'to',\n",
              " 'school',\n",
              " 'without',\n",
              " 'any',\n",
              " 'of',\n",
              " 'his',\n",
              " 'homework',\n",
              " 'done',\n",
              " 'the',\n",
              " 'dursleys',\n",
              " 'were',\n",
              " 'what',\n",
              " 'wizards',\n",
              " 'called',\n",
              " 'muggles',\n",
              " 'not',\n",
              " 'drop',\n",
              " 'of',\n",
              " 'magical',\n",
              " 'blood',\n",
              " 'in',\n",
              " 'their',\n",
              " 'veins',\n",
              " 'and',\n",
              " 'as',\n",
              " 'far',\n",
              " 'as',\n",
              " 'they',\n",
              " 'were',\n",
              " 'concerned',\n",
              " 'having',\n",
              " 'wizard',\n",
              " 'in',\n",
              " 'family',\n",
              " 'was',\n",
              " 'matter',\n",
              " 'of',\n",
              " 'deepest',\n",
              " 'shame',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'had',\n",
              " 'even',\n",
              " 'padlocked',\n",
              " 'harry',\n",
              " '’s',\n",
              " 'owl',\n",
              " 'hedwig',\n",
              " 'inside',\n",
              " 'her',\n",
              " 'cage',\n",
              " 'to',\n",
              " 'stop',\n",
              " 'her',\n",
              " 'from',\n",
              " 'carrying',\n",
              " 'messages',\n",
              " 'to',\n",
              " 'anyone',\n",
              " 'in',\n",
              " 'wizarding',\n",
              " 'world',\n",
              " 'harry',\n",
              " 'looked',\n",
              " 'nothing',\n",
              " 'like',\n",
              " 'rest',\n",
              " 'of',\n",
              " 'family',\n",
              " 'uncle',\n",
              " 'vernon',\n",
              " 'was',\n",
              " 'large',\n",
              " 'and',\n",
              " 'neckless',\n",
              " 'with',\n",
              " 'enormous',\n",
              " 'black',\n",
              " 'mustache',\n",
              " 'aunt',\n",
              " 'petunia',\n",
              " 'was',\n",
              " 'horse',\n",
              " 'faced',\n",
              " 'and',\n",
              " 'bony',\n",
              " 'dudley',\n",
              " 'was',\n",
              " 'blond',\n",
              " 'pink',\n",
              " 'and',\n",
              " 'porky',\n",
              " 'harry',\n",
              " 'on',\n",
              " 'other',\n",
              " 'hand',\n",
              " 'was',\n",
              " 'small',\n",
              " 'and',\n",
              " 'skinny',\n",
              " 'with',\n",
              " 'brilliant',\n",
              " 'green',\n",
              " 'eyes',\n",
              " 'and',\n",
              " 'jet',\n",
              " 'black',\n",
              " 'hair',\n",
              " 'that',\n",
              " 'was',\n",
              " 'always',\n",
              " 'untidy',\n",
              " 'he',\n",
              " 'wore',\n",
              " 'round',\n",
              " 'glasses',\n",
              " 'and',\n",
              " 'on',\n",
              " 'his',\n",
              " 'forehead',\n",
              " 'was',\n",
              " 'thin',\n",
              " 'lightning',\n",
              " 'shaped',\n",
              " 'scar',\n",
              " 'it',\n",
              " 'was',\n",
              " 'this',\n",
              " 'scar',\n",
              " 'that',\n",
              " 'made',\n",
              " 'harry',\n",
              " 'so',\n",
              " 'particularly',\n",
              " 'unusual',\n",
              " 'even',\n",
              " 'for',\n",
              " 'wizard',\n",
              " 'this',\n",
              " 'scar',\n",
              " 'was',\n",
              " 'only',\n",
              " 'hint',\n",
              " 'of',\n",
              " 'harry',\n",
              " '’s',\n",
              " 'very',\n",
              " 'mysterious',\n",
              " 'past',\n",
              " 'of',\n",
              " 'reason',\n",
              " 'he',\n",
              " 'had',\n",
              " 'been',\n",
              " 'left',\n",
              " 'on',\n",
              " 'dursleys',\n",
              " '’',\n",
              " 'doorstep',\n",
              " 'eleven',\n",
              " 'years',\n",
              " 'before',\n",
              " 'at',\n",
              " 'age',\n",
              " 'of',\n",
              " 'one',\n",
              " 'year',\n",
              " 'old',\n",
              " 'harry',\n",
              " 'had',\n",
              " 'somehow',\n",
              " 'survived',\n",
              " 'curse',\n",
              " 'from',\n",
              " 'greatest',\n",
              " 'dark',\n",
              " 'sorcerer',\n",
              " 'of',\n",
              " 'all',\n",
              " 'time',\n",
              " 'lord',\n",
              " 'voldemort',\n",
              " 'whose',\n",
              " 'name',\n",
              " 'most',\n",
              " 'witches',\n",
              " 'and',\n",
              " 'wizards',\n",
              " 'still',\n",
              " 'feared',\n",
              " 'to',\n",
              " 'speak',\n",
              " 'harry',\n",
              " '’s',\n",
              " 'parents',\n",
              " 'had',\n",
              " 'died',\n",
              " 'in',\n",
              " 'voldemort',\n",
              " '’s',\n",
              " 'attack',\n",
              " 'but',\n",
              " 'harry',\n",
              " 'had',\n",
              " 'escaped',\n",
              " 'with',\n",
              " 'his',\n",
              " 'lightning',\n",
              " 'scar',\n",
              " 'and',\n",
              " 'somehow',\n",
              " '—',\n",
              " 'nobody',\n",
              " 'understood',\n",
              " 'why',\n",
              " '—',\n",
              " 'voldemort',\n",
              " '’s',\n",
              " 'powers',\n",
              " 'had',\n",
              " 'been',\n",
              " 'destroyed',\n",
              " 'instant',\n",
              " 'he',\n",
              " 'had',\n",
              " 'failed',\n",
              " 'to',\n",
              " 'kill',\n",
              " 'harry',\n",
              " 'so',\n",
              " 'harry',\n",
              " 'had',\n",
              " 'been',\n",
              " 'brought',\n",
              " 'up',\n",
              " 'by',\n",
              " 'his',\n",
              " 'dead',\n",
              " 'mother',\n",
              " '’s',\n",
              " 'sister',\n",
              " 'and',\n",
              " 'her',\n",
              " 'husband',\n",
              " 'he',\n",
              " 'had',\n",
              " 'spent',\n",
              " 'ten',\n",
              " 'years',\n",
              " 'with',\n",
              " 'dursleys',\n",
              " 'never',\n",
              " 'understanding',\n",
              " 'why',\n",
              " 'he',\n",
              " 'kept',\n",
              " 'making',\n",
              " 'odd',\n",
              " 'things',\n",
              " 'happen',\n",
              " 'without',\n",
              " 'meaning',\n",
              " 'to',\n",
              " 'believing',\n",
              " 'dursleys',\n",
              " '’',\n",
              " 'story',\n",
              " 'that',\n",
              " 'he',\n",
              " 'had',\n",
              " 'got',\n",
              " 'his',\n",
              " 'scar',\n",
              " 'in',\n",
              " 'car',\n",
              " 'crash',\n",
              " 'that',\n",
              " 'had',\n",
              " 'killed',\n",
              " 'his',\n",
              " 'parents',\n",
              " 'and',\n",
              " 'then',\n",
              " 'exactly',\n",
              " 'year',\n",
              " 'ago',\n",
              " 'hogwarts',\n",
              " 'had',\n",
              " 'written',\n",
              " 'to',\n",
              " 'harry',\n",
              " 'and',\n",
              " 'whole',\n",
              " 'story',\n",
              " 'had',\n",
              " 'come',\n",
              " 'out',\n",
              " 'harry',\n",
              " 'had',\n",
              " 'taken',\n",
              " 'up',\n",
              " 'his',\n",
              " 'place',\n",
              " 'at',\n",
              " 'wizard',\n",
              " 'school',\n",
              " 'where',\n",
              " 'he',\n",
              " 'and',\n",
              " 'his',\n",
              " 'scar',\n",
              " 'were',\n",
              " 'famous',\n",
              " 'but',\n",
              " 'now',\n",
              " 'school',\n",
              " 'year',\n",
              " 'was',\n",
              " 'over',\n",
              " 'and',\n",
              " 'he',\n",
              " 'was',\n",
              " 'back',\n",
              " 'with',\n",
              " 'dursleys',\n",
              " 'for',\n",
              " 'summer',\n",
              " 'back',\n",
              " 'to',\n",
              " 'being',\n",
              " 'treated',\n",
              " 'like',\n",
              " 'dog',\n",
              " 'that',\n",
              " 'had',\n",
              " 'rolled',\n",
              " 'in',\n",
              " 'something',\n",
              " 'smelly',\n",
              " 'the',\n",
              " 'dursleys',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytpcbPT3Uqze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMGZgeqMUstQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts = Counter(tokens)\n",
        "#counts.most_common()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtaJ0QYYUt5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "353e1a32-401c-4e1b-ea60-8dfeb09af32c"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85739"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXz2irmRUvGH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb2bac57-5f1a-445f-fe39-d2dc202b4c4c"
      },
      "source": [
        "len(tokens)/30"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2857.9666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KCFaIY2UxdE",
        "colab_type": "text"
      },
      "source": [
        "## Create Sequences of Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPU2tZGUwLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# organize into sequences of tokens\n",
        "train_len = 30+1 # 30 training words , then one target word\n",
        "\n",
        "# Empty list of sequences\n",
        "text_sequences = []\n",
        "\n",
        "for i in range(train_len, len(tokens)):\n",
        "    \n",
        "    # Grab train_len# amount of characters\n",
        "    seq = tokens[i-train_len:i]\n",
        "    \n",
        "    # Add to list of sequences\n",
        "    text_sequences.append(seq)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYpCIv8dU1u7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98b5a6c2-e97e-4fa7-b73f-fd4b66daab83"
      },
      "source": [
        "' '.join(text_sequences[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'not for first time argument had broken out over breakfast at number four privet drive mr. vernon dursley had been woken in early hours of morning by loud hooting noise from'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_SrUSf9U3JQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cd97b2a-d9e6-448a-cdbe-0c964313c6cc"
      },
      "source": [
        "len(text_sequences)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qWJvMiGU6b2",
        "colab_type": "text"
      },
      "source": [
        "# Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsX1R5xvU9G9",
        "colab_type": "text"
      },
      "source": [
        "### Keras Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC01tp0CVC6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28938ac3-611b-407d-859f-04df31458bd5"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx053oVUVDf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz0xGQtzVE8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMtEAqMDVGZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sequences[0]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwceAdwdVHre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizer.index_word"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxhl4A4qVJF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "e2aafda1-8972-4487-d5eb-de0bfb0dec61"
      },
      "source": [
        "for i in sequences[0]:\n",
        "    print(f'{i} : {tokenizer.index_word[i]}')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31 : not\n",
            "30 : for\n",
            "177 : first\n",
            "89 : time\n",
            "3839 : argument\n",
            "16 : had\n",
            "818 : broken\n",
            "32 : out\n",
            "63 : over\n",
            "708 : breakfast\n",
            "20 : at\n",
            "1584 : number\n",
            "399 : four\n",
            "1383 : privet\n",
            "1382 : drive\n",
            "113 : mr.\n",
            "284 : vernon\n",
            "2203 : dursley\n",
            "16 : had\n",
            "49 : been\n",
            "1853 : woken\n",
            "14 : in\n",
            "1220 : early\n",
            "755 : hours\n",
            "5 : of\n",
            "528 : morning\n",
            "70 : by\n",
            "332 : loud\n",
            "6781 : hooting\n",
            "527 : noise\n",
            "42 : from\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3MQWhF1VLw0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd803839-07cd-49cd-8a8b-f49131e71f9e"
      },
      "source": [
        "vocabulary_size = len(tokenizer.word_counts)\n",
        "vocabulary_size"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6781"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFaqGAM-VObu",
        "colab_type": "text"
      },
      "source": [
        "### Convert to Numpy Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KUlAVDGVNBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViktYDmLVP3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = np.array(sequences)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrbwTEbBVQ9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "af853380-43f4-4359-8e69-34ae2ddcaec7"
      },
      "source": [
        "sequences"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  31,   30,  177, ..., 6781,  527,   42],\n",
              "       [  30,  177,   89, ...,  527,   42,   11],\n",
              "       [ 177,   89, 3839, ...,   42,   11, 2776],\n",
              "       ...,\n",
              "       [1578,  105,   20, ...,    6,  757,    3],\n",
              "       [ 105,   20,  130, ...,  757,    3,  674],\n",
              "       [  20,  130,    6, ...,    3,  674,  104]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlIPTlyLVZ9a",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Creating an LSTM based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrkzeogYVSOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,Embedding,Dropout"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsPeyKH6VT0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(vocabulary_size, seq_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
        "    model.add(LSTM(150, return_sequences=False))\n",
        "    model.add(Dense(150, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(150, activation='relu'))\n",
        "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmpOLJ4bVhuo",
        "colab_type": "text"
      },
      "source": [
        "### Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKBapXJkVgDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6cJRBe9VjN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7056f0e1-1bb4-4013-cfbd-71cb3349fc77"
      },
      "source": [
        "sequences"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  31,   30,  177, ..., 6781,  527,   42],\n",
              "       [  30,  177,   89, ...,  527,   42,   11],\n",
              "       [ 177,   89, 3839, ...,   42,   11, 2776],\n",
              "       ...,\n",
              "       [1578,  105,   20, ...,    6,  757,    3],\n",
              "       [ 105,   20,  130, ...,  757,    3,  674],\n",
              "       [  20,  130,    6, ...,    3,  674,  104]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O01C81t-VkWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "19d15de6-8409-4ba6-91d3-0385dd6d6c63"
      },
      "source": [
        "# First 49 words\n",
        "sequences[:,:-1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  31,   30,  177, ...,  332, 6781,  527],\n",
              "       [  30,  177,   89, ..., 6781,  527,   42],\n",
              "       [ 177,   89, 3839, ...,  527,   42,   11],\n",
              "       ...,\n",
              "       [1578,  105,   20, ...,  308,    6,  757],\n",
              "       [ 105,   20,  130, ...,    6,  757,    3],\n",
              "       [  20,  130,    6, ...,  757,    3,  674]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_SPtaKuVlld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61c08b59-651e-4885-bc70-6792f8c0f131"
      },
      "source": [
        "# last Word\n",
        "sequences[:,-1]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  42,   11, 2776, ...,    3,  674,  104])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njo6MATjVmiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = sequences[:,:-1]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIA8gB5eVncW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = sequences[:,-1]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D85tuxYYVoce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = to_categorical(y, num_classes=vocabulary_size+1 , dtype='uint8')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjRBYqPfVpcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = X.shape[1]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT1dBKyVVrJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "933078a5-bfdc-4319-e7b3-5e47b37efece"
      },
      "source": [
        "seq_len"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shYUr-97VsZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fade30e-3c10-4fac-a801-e2574909f595"
      },
      "source": [
        "sequences[:,-1]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  42,   11, 2776, ...,    3,  674,  104])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce7Ox0hV489e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8KaPC3y5Hvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tWIHnA4VvDw",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo-yFexZVtWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ad913e02-4f98-450c-8ba5-3f595a25a233"
      },
      "source": [
        "# define model\n",
        "model = create_model(vocabulary_size+1, seq_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 30, 25)            169550    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 150)               105600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 150)               22650     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 150)               22650     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6782)              1024082   \n",
            "=================================================================\n",
            "Total params: 1,344,532\n",
            "Trainable params: 1,344,532\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hHoMszqVymQ",
        "colab_type": "text"
      },
      "source": [
        "# Train Again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY5gfY8sV3Wz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9595577e-9a0c-4c15-9bff-1ff3b0c7d6e4"
      },
      "source": [
        "# fit model\n",
        "train_acc=list()\n",
        "train_loss=list()\n",
        "val_acc = list()\n",
        "val_loss=list()\n",
        "history = model.fit(X_train, y_train, batch_size=128 ,validation_data=(X_test, y_test), epochs = 400, verbose=1)\n",
        "\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss= history.history['loss']\n",
        "val_acc   = history.history['val_accuracy']\n",
        "val_loss  = history.history['val_loss']\n",
        "\n",
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/val_loss.txt\", \"w\") as file:\n",
        "    file.write(str(val_loss))\n",
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/val_accuracy.txt\", \"w\") as file:\n",
        "    file.write(str(val_acc))\n",
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/accuracy.txt\", \"w\") as file:\n",
        "    file.write(str(train_acc))\n",
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/loss.txt\", \"w\") as file:\n",
        "    file.write(str(train_loss))    \n",
        "\n",
        "model.save('/gdrive/My Drive/Colab Notebooks/Dataset/Next text/text_generation.h5') \n",
        "import pickle\n",
        "with open('/gdrive/My Drive/Colab Notebooks/Dataset/Next text/300620.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 57424 samples, validate on 28284 samples\n",
            "Epoch 1/400\n",
            "57424/57424 [==============================] - 32s 563us/step - loss: 6.7640 - accuracy: 0.0318 - val_loss: 6.7068 - val_accuracy: 0.0330\n",
            "Epoch 2/400\n",
            "57424/57424 [==============================] - 29s 505us/step - loss: 6.4626 - accuracy: 0.0339 - val_loss: 6.6055 - val_accuracy: 0.0359\n",
            "Epoch 3/400\n",
            "57424/57424 [==============================] - 29s 507us/step - loss: 6.3550 - accuracy: 0.0401 - val_loss: 6.5016 - val_accuracy: 0.0460\n",
            "Epoch 4/400\n",
            "57424/57424 [==============================] - 31s 546us/step - loss: 6.2028 - accuracy: 0.0513 - val_loss: 6.4189 - val_accuracy: 0.0588\n",
            "Epoch 5/400\n",
            "57424/57424 [==============================] - 29s 512us/step - loss: 6.0317 - accuracy: 0.0676 - val_loss: 6.2814 - val_accuracy: 0.0820\n",
            "Epoch 6/400\n",
            "57424/57424 [==============================] - 30s 518us/step - loss: 5.8410 - accuracy: 0.0896 - val_loss: 6.2097 - val_accuracy: 0.0951\n",
            "Epoch 7/400\n",
            "57424/57424 [==============================] - 31s 540us/step - loss: 5.6885 - accuracy: 0.1018 - val_loss: 6.1720 - val_accuracy: 0.1010\n",
            "Epoch 8/400\n",
            "57424/57424 [==============================] - 31s 534us/step - loss: 5.5679 - accuracy: 0.1100 - val_loss: 6.1713 - val_accuracy: 0.1076\n",
            "Epoch 9/400\n",
            "57424/57424 [==============================] - 30s 519us/step - loss: 5.5276 - accuracy: 0.1122 - val_loss: 6.2078 - val_accuracy: 0.1058\n",
            "Epoch 10/400\n",
            "57424/57424 [==============================] - 31s 536us/step - loss: 5.3746 - accuracy: 0.1243 - val_loss: 6.1836 - val_accuracy: 0.1169\n",
            "Epoch 11/400\n",
            "57424/57424 [==============================] - 30s 521us/step - loss: 5.3037 - accuracy: 0.1283 - val_loss: 6.1998 - val_accuracy: 0.1204\n",
            "Epoch 12/400\n",
            "57424/57424 [==============================] - 30s 521us/step - loss: 5.1959 - accuracy: 0.1332 - val_loss: 6.2405 - val_accuracy: 0.1210\n",
            "Epoch 13/400\n",
            "57424/57424 [==============================] - 30s 528us/step - loss: 5.1196 - accuracy: 0.1372 - val_loss: 6.2913 - val_accuracy: 0.1216\n",
            "Epoch 14/400\n",
            "57424/57424 [==============================] - 31s 538us/step - loss: 5.0527 - accuracy: 0.1414 - val_loss: 6.3103 - val_accuracy: 0.1228\n",
            "Epoch 15/400\n",
            "57424/57424 [==============================] - 31s 532us/step - loss: 4.9881 - accuracy: 0.1450 - val_loss: 6.3962 - val_accuracy: 0.1240\n",
            "Epoch 16/400\n",
            "57424/57424 [==============================] - 30s 518us/step - loss: 4.9272 - accuracy: 0.1479 - val_loss: 6.4544 - val_accuracy: 0.1261\n",
            "Epoch 17/400\n",
            "57424/57424 [==============================] - 31s 548us/step - loss: 4.8764 - accuracy: 0.1509 - val_loss: 6.5188 - val_accuracy: 0.1264\n",
            "Epoch 18/400\n",
            "57424/57424 [==============================] - 32s 552us/step - loss: 4.8254 - accuracy: 0.1539 - val_loss: 6.5441 - val_accuracy: 0.1258\n",
            "Epoch 19/400\n",
            "57424/57424 [==============================] - 31s 537us/step - loss: 4.7757 - accuracy: 0.1558 - val_loss: 6.6727 - val_accuracy: 0.1271\n",
            "Epoch 20/400\n",
            "57424/57424 [==============================] - 30s 530us/step - loss: 4.7262 - accuracy: 0.1583 - val_loss: 6.7390 - val_accuracy: 0.1288\n",
            "Epoch 21/400\n",
            "57424/57424 [==============================] - 31s 546us/step - loss: 4.6803 - accuracy: 0.1598 - val_loss: 6.8232 - val_accuracy: 0.1289\n",
            "Epoch 22/400\n",
            "57424/57424 [==============================] - 31s 535us/step - loss: 4.6360 - accuracy: 0.1639 - val_loss: 6.8824 - val_accuracy: 0.1311\n",
            "Epoch 23/400\n",
            "57424/57424 [==============================] - 30s 517us/step - loss: 4.5963 - accuracy: 0.1661 - val_loss: 6.9515 - val_accuracy: 0.1288\n",
            "Epoch 24/400\n",
            "57424/57424 [==============================] - 29s 502us/step - loss: 4.5504 - accuracy: 0.1678 - val_loss: 7.0880 - val_accuracy: 0.1288\n",
            "Epoch 25/400\n",
            "57424/57424 [==============================] - 29s 508us/step - loss: 4.5165 - accuracy: 0.1703 - val_loss: 7.1784 - val_accuracy: 0.1290\n",
            "Epoch 26/400\n",
            "57424/57424 [==============================] - 30s 528us/step - loss: 4.4737 - accuracy: 0.1734 - val_loss: 7.2169 - val_accuracy: 0.1317\n",
            "Epoch 27/400\n",
            "57424/57424 [==============================] - 29s 509us/step - loss: 4.4368 - accuracy: 0.1746 - val_loss: 7.3167 - val_accuracy: 0.1320\n",
            "Epoch 28/400\n",
            "57424/57424 [==============================] - 31s 534us/step - loss: 4.4042 - accuracy: 0.1762 - val_loss: 7.3762 - val_accuracy: 0.1315\n",
            "Epoch 29/400\n",
            "57424/57424 [==============================] - 30s 527us/step - loss: 4.3630 - accuracy: 0.1796 - val_loss: 7.5191 - val_accuracy: 0.1313\n",
            "Epoch 30/400\n",
            "57424/57424 [==============================] - 30s 518us/step - loss: 4.3302 - accuracy: 0.1832 - val_loss: 7.6176 - val_accuracy: 0.1294\n",
            "Epoch 31/400\n",
            "57424/57424 [==============================] - 30s 514us/step - loss: 4.2946 - accuracy: 0.1834 - val_loss: 7.6443 - val_accuracy: 0.1300\n",
            "Epoch 32/400\n",
            "57424/57424 [==============================] - 30s 527us/step - loss: 4.2631 - accuracy: 0.1849 - val_loss: 7.7713 - val_accuracy: 0.1310\n",
            "Epoch 33/400\n",
            "57424/57424 [==============================] - 29s 514us/step - loss: 4.2325 - accuracy: 0.1874 - val_loss: 7.9518 - val_accuracy: 0.1302\n",
            "Epoch 34/400\n",
            "57424/57424 [==============================] - 29s 512us/step - loss: 4.1971 - accuracy: 0.1909 - val_loss: 7.9920 - val_accuracy: 0.1289\n",
            "Epoch 35/400\n",
            "57424/57424 [==============================] - 29s 500us/step - loss: 4.1680 - accuracy: 0.1929 - val_loss: 8.1096 - val_accuracy: 0.1311\n",
            "Epoch 36/400\n",
            "57424/57424 [==============================] - 30s 515us/step - loss: 4.1358 - accuracy: 0.1947 - val_loss: 8.1814 - val_accuracy: 0.1301\n",
            "Epoch 37/400\n",
            "57424/57424 [==============================] - 30s 529us/step - loss: 4.1029 - accuracy: 0.1987 - val_loss: 8.2533 - val_accuracy: 0.1315\n",
            "Epoch 38/400\n",
            "57424/57424 [==============================] - 31s 542us/step - loss: 4.0802 - accuracy: 0.1987 - val_loss: 8.3443 - val_accuracy: 0.1331\n",
            "Epoch 39/400\n",
            "57424/57424 [==============================] - 31s 534us/step - loss: 4.0468 - accuracy: 0.2011 - val_loss: 8.4293 - val_accuracy: 0.1312\n",
            "Epoch 40/400\n",
            "57424/57424 [==============================] - 31s 535us/step - loss: 4.0259 - accuracy: 0.2033 - val_loss: 8.5741 - val_accuracy: 0.1264\n",
            "Epoch 41/400\n",
            "57424/57424 [==============================] - 31s 531us/step - loss: 3.9956 - accuracy: 0.2052 - val_loss: 8.5510 - val_accuracy: 0.1319\n",
            "Epoch 42/400\n",
            "57424/57424 [==============================] - 31s 544us/step - loss: 3.9658 - accuracy: 0.2091 - val_loss: 8.6580 - val_accuracy: 0.1288\n",
            "Epoch 43/400\n",
            "57424/57424 [==============================] - 30s 520us/step - loss: 3.9423 - accuracy: 0.2101 - val_loss: 8.7938 - val_accuracy: 0.1289\n",
            "Epoch 44/400\n",
            "57424/57424 [==============================] - 30s 522us/step - loss: 3.9069 - accuracy: 0.2123 - val_loss: 8.9745 - val_accuracy: 0.1294\n",
            "Epoch 45/400\n",
            "57424/57424 [==============================] - 29s 506us/step - loss: 3.8783 - accuracy: 0.2147 - val_loss: 8.9542 - val_accuracy: 0.1286\n",
            "Epoch 46/400\n",
            "57424/57424 [==============================] - 29s 511us/step - loss: 3.8549 - accuracy: 0.2166 - val_loss: 9.0874 - val_accuracy: 0.1297\n",
            "Epoch 47/400\n",
            "57424/57424 [==============================] - 30s 525us/step - loss: 3.8314 - accuracy: 0.2185 - val_loss: 9.1185 - val_accuracy: 0.1269\n",
            "Epoch 48/400\n",
            "57424/57424 [==============================] - 31s 540us/step - loss: 3.7962 - accuracy: 0.2216 - val_loss: 9.2426 - val_accuracy: 0.1280\n",
            "Epoch 49/400\n",
            "57424/57424 [==============================] - 30s 523us/step - loss: 3.7744 - accuracy: 0.2241 - val_loss: 9.4007 - val_accuracy: 0.1255\n",
            "Epoch 50/400\n",
            "57424/57424 [==============================] - 30s 514us/step - loss: 3.7529 - accuracy: 0.2262 - val_loss: 9.3745 - val_accuracy: 0.1251\n",
            "Epoch 51/400\n",
            "57424/57424 [==============================] - 29s 511us/step - loss: 3.7238 - accuracy: 0.2294 - val_loss: 9.5864 - val_accuracy: 0.1256\n",
            "Epoch 52/400\n",
            "57424/57424 [==============================] - 30s 515us/step - loss: 3.6994 - accuracy: 0.2298 - val_loss: 9.5520 - val_accuracy: 0.1252\n",
            "Epoch 53/400\n",
            "57424/57424 [==============================] - 31s 545us/step - loss: 3.6784 - accuracy: 0.2307 - val_loss: 9.7441 - val_accuracy: 0.1250\n",
            "Epoch 54/400\n",
            "57424/57424 [==============================] - 29s 506us/step - loss: 3.6535 - accuracy: 0.2360 - val_loss: 9.8297 - val_accuracy: 0.1251\n",
            "Epoch 55/400\n",
            "57424/57424 [==============================] - 30s 521us/step - loss: 3.6317 - accuracy: 0.2368 - val_loss: 9.8257 - val_accuracy: 0.1257\n",
            "Epoch 56/400\n",
            "57424/57424 [==============================] - 30s 514us/step - loss: 3.6111 - accuracy: 0.2409 - val_loss: 9.8928 - val_accuracy: 0.1240\n",
            "Epoch 57/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 3.5931 - accuracy: 0.2408 - val_loss: 9.9702 - val_accuracy: 0.1232\n",
            "Epoch 58/400\n",
            "57424/57424 [==============================] - 30s 530us/step - loss: 3.5746 - accuracy: 0.2449 - val_loss: 10.0347 - val_accuracy: 0.1248\n",
            "Epoch 59/400\n",
            "57424/57424 [==============================] - 30s 520us/step - loss: 3.5468 - accuracy: 0.2459 - val_loss: 10.1558 - val_accuracy: 0.1243\n",
            "Epoch 60/400\n",
            "57424/57424 [==============================] - 30s 525us/step - loss: 3.5269 - accuracy: 0.2492 - val_loss: 10.2570 - val_accuracy: 0.1223\n",
            "Epoch 61/400\n",
            "57424/57424 [==============================] - 30s 521us/step - loss: 3.5089 - accuracy: 0.2507 - val_loss: 10.3455 - val_accuracy: 0.1237\n",
            "Epoch 62/400\n",
            "57424/57424 [==============================] - 30s 518us/step - loss: 3.4948 - accuracy: 0.2513 - val_loss: 10.3915 - val_accuracy: 0.1216\n",
            "Epoch 63/400\n",
            "57424/57424 [==============================] - 30s 522us/step - loss: 3.4676 - accuracy: 0.2554 - val_loss: 10.5012 - val_accuracy: 0.1213\n",
            "Epoch 64/400\n",
            "57424/57424 [==============================] - 30s 530us/step - loss: 3.4479 - accuracy: 0.2581 - val_loss: 10.5273 - val_accuracy: 0.1217\n",
            "Epoch 65/400\n",
            "57424/57424 [==============================] - 31s 540us/step - loss: 3.4315 - accuracy: 0.2595 - val_loss: 10.6150 - val_accuracy: 0.1223\n",
            "Epoch 66/400\n",
            "57424/57424 [==============================] - 31s 533us/step - loss: 3.4070 - accuracy: 0.2622 - val_loss: 10.6890 - val_accuracy: 0.1209\n",
            "Epoch 67/400\n",
            "57424/57424 [==============================] - 29s 505us/step - loss: 3.3843 - accuracy: 0.2630 - val_loss: 10.7824 - val_accuracy: 0.1216\n",
            "Epoch 68/400\n",
            "57424/57424 [==============================] - 29s 513us/step - loss: 3.3662 - accuracy: 0.2671 - val_loss: 10.8017 - val_accuracy: 0.1189\n",
            "Epoch 69/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 3.3536 - accuracy: 0.2694 - val_loss: 10.9206 - val_accuracy: 0.1190\n",
            "Epoch 70/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 3.3272 - accuracy: 0.2706 - val_loss: 11.0781 - val_accuracy: 0.1178\n",
            "Epoch 71/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 3.3183 - accuracy: 0.2741 - val_loss: 11.1149 - val_accuracy: 0.1189\n",
            "Epoch 72/400\n",
            "57424/57424 [==============================] - 29s 513us/step - loss: 3.2922 - accuracy: 0.2762 - val_loss: 11.1458 - val_accuracy: 0.1172\n",
            "Epoch 73/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 3.2772 - accuracy: 0.2759 - val_loss: 11.2836 - val_accuracy: 0.1172\n",
            "Epoch 74/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 3.2671 - accuracy: 0.2791 - val_loss: 11.3941 - val_accuracy: 0.1161\n",
            "Epoch 75/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 3.2584 - accuracy: 0.2817 - val_loss: 11.2653 - val_accuracy: 0.1158\n",
            "Epoch 76/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 3.2362 - accuracy: 0.2823 - val_loss: 11.3487 - val_accuracy: 0.1148\n",
            "Epoch 77/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 3.2208 - accuracy: 0.2864 - val_loss: 11.4594 - val_accuracy: 0.1162\n",
            "Epoch 78/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 3.2062 - accuracy: 0.2873 - val_loss: 11.5547 - val_accuracy: 0.1143\n",
            "Epoch 79/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 3.1767 - accuracy: 0.2899 - val_loss: 11.6200 - val_accuracy: 0.1121\n",
            "Epoch 80/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 3.1618 - accuracy: 0.2916 - val_loss: 11.6810 - val_accuracy: 0.1099\n",
            "Epoch 81/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 3.1487 - accuracy: 0.2955 - val_loss: 11.7748 - val_accuracy: 0.1119\n",
            "Epoch 82/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 3.1346 - accuracy: 0.2971 - val_loss: 11.7678 - val_accuracy: 0.1124\n",
            "Epoch 83/400\n",
            "57424/57424 [==============================] - 29s 513us/step - loss: 3.1229 - accuracy: 0.3000 - val_loss: 11.9882 - val_accuracy: 0.1117\n",
            "Epoch 84/400\n",
            "57424/57424 [==============================] - 29s 504us/step - loss: 3.1142 - accuracy: 0.3014 - val_loss: 11.9031 - val_accuracy: 0.1120\n",
            "Epoch 85/400\n",
            "57424/57424 [==============================] - 30s 518us/step - loss: 3.0816 - accuracy: 0.3043 - val_loss: 12.0765 - val_accuracy: 0.1096\n",
            "Epoch 86/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 3.0774 - accuracy: 0.3056 - val_loss: 12.0888 - val_accuracy: 0.1077\n",
            "Epoch 87/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 3.0692 - accuracy: 0.3039 - val_loss: 12.2167 - val_accuracy: 0.1101\n",
            "Epoch 88/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 3.0519 - accuracy: 0.3096 - val_loss: 12.2073 - val_accuracy: 0.1078\n",
            "Epoch 89/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 3.0423 - accuracy: 0.3076 - val_loss: 12.1190 - val_accuracy: 0.1082\n",
            "Epoch 90/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 3.0168 - accuracy: 0.3147 - val_loss: 12.2207 - val_accuracy: 0.1086\n",
            "Epoch 91/400\n",
            "57424/57424 [==============================] - 29s 500us/step - loss: 2.9973 - accuracy: 0.3160 - val_loss: 12.3418 - val_accuracy: 0.1062\n",
            "Epoch 92/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 2.9876 - accuracy: 0.3180 - val_loss: 12.5540 - val_accuracy: 0.1069\n",
            "Epoch 93/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 2.9815 - accuracy: 0.3194 - val_loss: 12.4253 - val_accuracy: 0.1077\n",
            "Epoch 94/400\n",
            "57424/57424 [==============================] - 28s 489us/step - loss: 2.9490 - accuracy: 0.3226 - val_loss: 12.6417 - val_accuracy: 0.1064\n",
            "Epoch 95/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 2.9541 - accuracy: 0.3228 - val_loss: 12.7169 - val_accuracy: 0.1050\n",
            "Epoch 96/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 2.9456 - accuracy: 0.3236 - val_loss: 12.6425 - val_accuracy: 0.1043\n",
            "Epoch 97/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 2.9284 - accuracy: 0.3245 - val_loss: 12.6975 - val_accuracy: 0.1054\n",
            "Epoch 98/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 2.9100 - accuracy: 0.3301 - val_loss: 12.7567 - val_accuracy: 0.1050\n",
            "Epoch 99/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 2.8947 - accuracy: 0.3313 - val_loss: 12.8502 - val_accuracy: 0.1046\n",
            "Epoch 100/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 2.8864 - accuracy: 0.3336 - val_loss: 13.0023 - val_accuracy: 0.1051\n",
            "Epoch 101/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 2.8722 - accuracy: 0.3345 - val_loss: 12.9071 - val_accuracy: 0.1043\n",
            "Epoch 102/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 2.8520 - accuracy: 0.3374 - val_loss: 13.1253 - val_accuracy: 0.1030\n",
            "Epoch 103/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.8566 - accuracy: 0.3383 - val_loss: 13.0902 - val_accuracy: 0.1056\n",
            "Epoch 104/400\n",
            "57424/57424 [==============================] - 29s 500us/step - loss: 2.8363 - accuracy: 0.3422 - val_loss: 13.0501 - val_accuracy: 0.1025\n",
            "Epoch 105/400\n",
            "57424/57424 [==============================] - 29s 496us/step - loss: 2.8231 - accuracy: 0.3433 - val_loss: 13.2593 - val_accuracy: 0.1040\n",
            "Epoch 106/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 2.8021 - accuracy: 0.3444 - val_loss: 13.3243 - val_accuracy: 0.0999\n",
            "Epoch 107/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 2.7950 - accuracy: 0.3460 - val_loss: 13.3495 - val_accuracy: 0.0993\n",
            "Epoch 108/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 2.7831 - accuracy: 0.3487 - val_loss: 13.3240 - val_accuracy: 0.0987\n",
            "Epoch 109/400\n",
            "57424/57424 [==============================] - 29s 502us/step - loss: 2.7748 - accuracy: 0.3503 - val_loss: 13.4719 - val_accuracy: 0.0990\n",
            "Epoch 110/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 2.7631 - accuracy: 0.3517 - val_loss: 13.3397 - val_accuracy: 0.1002\n",
            "Epoch 111/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 2.7457 - accuracy: 0.3550 - val_loss: 13.5162 - val_accuracy: 0.0985\n",
            "Epoch 112/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 2.7368 - accuracy: 0.3554 - val_loss: 13.5955 - val_accuracy: 0.0992\n",
            "Epoch 113/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 2.7313 - accuracy: 0.3563 - val_loss: 13.5695 - val_accuracy: 0.0987\n",
            "Epoch 114/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 2.7191 - accuracy: 0.3594 - val_loss: 13.6866 - val_accuracy: 0.0991\n",
            "Epoch 115/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 2.7011 - accuracy: 0.3609 - val_loss: 13.6846 - val_accuracy: 0.0975\n",
            "Epoch 116/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 2.6931 - accuracy: 0.3634 - val_loss: 13.8729 - val_accuracy: 0.0984\n",
            "Epoch 117/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 2.6755 - accuracy: 0.3669 - val_loss: 13.7916 - val_accuracy: 0.0963\n",
            "Epoch 118/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 2.6748 - accuracy: 0.3652 - val_loss: 13.8266 - val_accuracy: 0.0979\n",
            "Epoch 119/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 2.6562 - accuracy: 0.3677 - val_loss: 13.9388 - val_accuracy: 0.0963\n",
            "Epoch 120/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 2.6595 - accuracy: 0.3696 - val_loss: 13.9642 - val_accuracy: 0.0956\n",
            "Epoch 121/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.6329 - accuracy: 0.3714 - val_loss: 14.0385 - val_accuracy: 0.0940\n",
            "Epoch 122/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 2.6140 - accuracy: 0.3751 - val_loss: 14.1325 - val_accuracy: 0.0957\n",
            "Epoch 123/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 2.6219 - accuracy: 0.3740 - val_loss: 14.1215 - val_accuracy: 0.0955\n",
            "Epoch 124/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 2.6092 - accuracy: 0.3783 - val_loss: 14.1355 - val_accuracy: 0.0942\n",
            "Epoch 125/400\n",
            "57424/57424 [==============================] - 28s 479us/step - loss: 2.6021 - accuracy: 0.3781 - val_loss: 14.2439 - val_accuracy: 0.0963\n",
            "Epoch 126/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 2.5934 - accuracy: 0.3799 - val_loss: 14.3256 - val_accuracy: 0.0930\n",
            "Epoch 127/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 2.5712 - accuracy: 0.3822 - val_loss: 14.3808 - val_accuracy: 0.0942\n",
            "Epoch 128/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 2.5690 - accuracy: 0.3848 - val_loss: 14.4201 - val_accuracy: 0.0940\n",
            "Epoch 129/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 2.5743 - accuracy: 0.3841 - val_loss: 14.4734 - val_accuracy: 0.0953\n",
            "Epoch 130/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 2.5541 - accuracy: 0.3876 - val_loss: 14.4345 - val_accuracy: 0.0908\n",
            "Epoch 131/400\n",
            "57424/57424 [==============================] - 27s 478us/step - loss: 2.5293 - accuracy: 0.3900 - val_loss: 14.5677 - val_accuracy: 0.0908\n",
            "Epoch 132/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 2.5335 - accuracy: 0.3925 - val_loss: 14.5291 - val_accuracy: 0.0940\n",
            "Epoch 133/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 2.5281 - accuracy: 0.3908 - val_loss: 14.6460 - val_accuracy: 0.0948\n",
            "Epoch 134/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 2.5107 - accuracy: 0.3939 - val_loss: 14.6908 - val_accuracy: 0.0901\n",
            "Epoch 135/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 2.4948 - accuracy: 0.3967 - val_loss: 14.8478 - val_accuracy: 0.0918\n",
            "Epoch 136/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 2.4880 - accuracy: 0.3974 - val_loss: 14.7880 - val_accuracy: 0.0934\n",
            "Epoch 137/400\n",
            "57424/57424 [==============================] - 29s 500us/step - loss: 2.4852 - accuracy: 0.3983 - val_loss: 14.8636 - val_accuracy: 0.0906\n",
            "Epoch 138/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 2.4696 - accuracy: 0.3998 - val_loss: 14.8928 - val_accuracy: 0.0900\n",
            "Epoch 139/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 2.4730 - accuracy: 0.4016 - val_loss: 14.9224 - val_accuracy: 0.0893\n",
            "Epoch 140/400\n",
            "57424/57424 [==============================] - 29s 502us/step - loss: 2.4655 - accuracy: 0.4025 - val_loss: 14.8999 - val_accuracy: 0.0893\n",
            "Epoch 141/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 2.4457 - accuracy: 0.4073 - val_loss: 15.1526 - val_accuracy: 0.0900\n",
            "Epoch 142/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 2.4362 - accuracy: 0.4079 - val_loss: 15.1232 - val_accuracy: 0.0908\n",
            "Epoch 143/400\n",
            "57424/57424 [==============================] - 29s 502us/step - loss: 2.4316 - accuracy: 0.4086 - val_loss: 15.3599 - val_accuracy: 0.0876\n",
            "Epoch 144/400\n",
            "57424/57424 [==============================] - 29s 509us/step - loss: 2.4280 - accuracy: 0.4098 - val_loss: 15.1602 - val_accuracy: 0.0877\n",
            "Epoch 145/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 2.4127 - accuracy: 0.4121 - val_loss: 15.2904 - val_accuracy: 0.0907\n",
            "Epoch 146/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 2.4071 - accuracy: 0.4113 - val_loss: 15.3217 - val_accuracy: 0.0908\n",
            "Epoch 147/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 2.4129 - accuracy: 0.4109 - val_loss: 15.3291 - val_accuracy: 0.0867\n",
            "Epoch 148/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 2.3764 - accuracy: 0.4189 - val_loss: 15.4266 - val_accuracy: 0.0876\n",
            "Epoch 149/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 2.3837 - accuracy: 0.4151 - val_loss: 15.3294 - val_accuracy: 0.0896\n",
            "Epoch 150/400\n",
            "57424/57424 [==============================] - 29s 501us/step - loss: 2.3833 - accuracy: 0.4165 - val_loss: 15.4786 - val_accuracy: 0.0885\n",
            "Epoch 151/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 2.3684 - accuracy: 0.4189 - val_loss: 15.4561 - val_accuracy: 0.0881\n",
            "Epoch 152/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 2.3634 - accuracy: 0.4187 - val_loss: 15.6183 - val_accuracy: 0.0885\n",
            "Epoch 153/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.3410 - accuracy: 0.4234 - val_loss: 15.6934 - val_accuracy: 0.0862\n",
            "Epoch 154/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 2.3508 - accuracy: 0.4212 - val_loss: 15.7508 - val_accuracy: 0.0875\n",
            "Epoch 155/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 2.3404 - accuracy: 0.4232 - val_loss: 15.7541 - val_accuracy: 0.0883\n",
            "Epoch 156/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 2.3433 - accuracy: 0.4257 - val_loss: 15.7706 - val_accuracy: 0.0853\n",
            "Epoch 157/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.3314 - accuracy: 0.4265 - val_loss: 15.8030 - val_accuracy: 0.0843\n",
            "Epoch 158/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 2.3249 - accuracy: 0.4303 - val_loss: 15.7966 - val_accuracy: 0.0846\n",
            "Epoch 159/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.3041 - accuracy: 0.4313 - val_loss: 15.9841 - val_accuracy: 0.0875\n",
            "Epoch 160/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.3155 - accuracy: 0.4303 - val_loss: 15.8745 - val_accuracy: 0.0878\n",
            "Epoch 161/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 2.3088 - accuracy: 0.4309 - val_loss: 15.8979 - val_accuracy: 0.0855\n",
            "Epoch 162/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.3038 - accuracy: 0.4335 - val_loss: 15.8675 - val_accuracy: 0.0846\n",
            "Epoch 163/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.2822 - accuracy: 0.4367 - val_loss: 16.0845 - val_accuracy: 0.0843\n",
            "Epoch 164/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 2.2883 - accuracy: 0.4358 - val_loss: 16.2044 - val_accuracy: 0.0868\n",
            "Epoch 165/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 2.2794 - accuracy: 0.4381 - val_loss: 15.9784 - val_accuracy: 0.0854\n",
            "Epoch 166/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 2.2812 - accuracy: 0.4372 - val_loss: 15.9327 - val_accuracy: 0.0834\n",
            "Epoch 167/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 2.2409 - accuracy: 0.4423 - val_loss: 16.1987 - val_accuracy: 0.0827\n",
            "Epoch 168/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.2498 - accuracy: 0.4423 - val_loss: 16.1818 - val_accuracy: 0.0831\n",
            "Epoch 169/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 2.2526 - accuracy: 0.4432 - val_loss: 16.2076 - val_accuracy: 0.0808\n",
            "Epoch 170/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 2.2438 - accuracy: 0.4420 - val_loss: 16.1468 - val_accuracy: 0.0820\n",
            "Epoch 171/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 2.2378 - accuracy: 0.4452 - val_loss: 16.3217 - val_accuracy: 0.0817\n",
            "Epoch 172/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 2.2160 - accuracy: 0.4476 - val_loss: 16.3529 - val_accuracy: 0.0825\n",
            "Epoch 173/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 2.2212 - accuracy: 0.4486 - val_loss: 16.3713 - val_accuracy: 0.0828\n",
            "Epoch 174/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 2.2228 - accuracy: 0.4508 - val_loss: 16.4124 - val_accuracy: 0.0838\n",
            "Epoch 175/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 2.1953 - accuracy: 0.4510 - val_loss: 16.4195 - val_accuracy: 0.0819\n",
            "Epoch 176/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 2.2246 - accuracy: 0.4479 - val_loss: 16.2600 - val_accuracy: 0.0813\n",
            "Epoch 177/400\n",
            "57424/57424 [==============================] - 27s 478us/step - loss: 2.1922 - accuracy: 0.4528 - val_loss: 16.7034 - val_accuracy: 0.0817\n",
            "Epoch 178/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 2.1893 - accuracy: 0.4564 - val_loss: 16.5647 - val_accuracy: 0.0827\n",
            "Epoch 179/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.1921 - accuracy: 0.4516 - val_loss: 16.4740 - val_accuracy: 0.0826\n",
            "Epoch 180/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 2.1813 - accuracy: 0.4569 - val_loss: 16.6831 - val_accuracy: 0.0814\n",
            "Epoch 181/400\n",
            "57424/57424 [==============================] - 29s 502us/step - loss: 2.1685 - accuracy: 0.4588 - val_loss: 16.6027 - val_accuracy: 0.0820\n",
            "Epoch 182/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 2.1722 - accuracy: 0.4585 - val_loss: 16.6374 - val_accuracy: 0.0808\n",
            "Epoch 183/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 2.1663 - accuracy: 0.4585 - val_loss: 16.7658 - val_accuracy: 0.0845\n",
            "Epoch 184/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 2.1466 - accuracy: 0.4636 - val_loss: 16.7614 - val_accuracy: 0.0816\n",
            "Epoch 185/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 2.1499 - accuracy: 0.4614 - val_loss: 16.7913 - val_accuracy: 0.0813\n",
            "Epoch 186/400\n",
            "57424/57424 [==============================] - 29s 500us/step - loss: 2.1379 - accuracy: 0.4643 - val_loss: 16.7393 - val_accuracy: 0.0824\n",
            "Epoch 187/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 2.1408 - accuracy: 0.4662 - val_loss: 16.9783 - val_accuracy: 0.0823\n",
            "Epoch 188/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 2.1396 - accuracy: 0.4656 - val_loss: 16.9678 - val_accuracy: 0.0799\n",
            "Epoch 189/400\n",
            "57424/57424 [==============================] - 28s 489us/step - loss: 2.1189 - accuracy: 0.4699 - val_loss: 17.0519 - val_accuracy: 0.0808\n",
            "Epoch 190/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 2.1210 - accuracy: 0.4694 - val_loss: 16.9865 - val_accuracy: 0.0809\n",
            "Epoch 191/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 2.1204 - accuracy: 0.4674 - val_loss: 17.0008 - val_accuracy: 0.0791\n",
            "Epoch 192/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 2.1125 - accuracy: 0.4709 - val_loss: 17.0496 - val_accuracy: 0.0792\n",
            "Epoch 193/400\n",
            "57424/57424 [==============================] - 27s 473us/step - loss: 2.1109 - accuracy: 0.4721 - val_loss: 17.1398 - val_accuracy: 0.0796\n",
            "Epoch 194/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 2.0967 - accuracy: 0.4700 - val_loss: 17.0302 - val_accuracy: 0.0810\n",
            "Epoch 195/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 2.0984 - accuracy: 0.4737 - val_loss: 17.3047 - val_accuracy: 0.0806\n",
            "Epoch 196/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 2.0975 - accuracy: 0.4715 - val_loss: 17.0820 - val_accuracy: 0.0772\n",
            "Epoch 197/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 2.0767 - accuracy: 0.4765 - val_loss: 17.2751 - val_accuracy: 0.0780\n",
            "Epoch 198/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 2.0811 - accuracy: 0.4746 - val_loss: 17.3289 - val_accuracy: 0.0805\n",
            "Epoch 199/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 2.0660 - accuracy: 0.4800 - val_loss: 17.2850 - val_accuracy: 0.0781\n",
            "Epoch 200/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 2.0670 - accuracy: 0.4790 - val_loss: 17.2153 - val_accuracy: 0.0755\n",
            "Epoch 201/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 2.0696 - accuracy: 0.4801 - val_loss: 17.1021 - val_accuracy: 0.0771\n",
            "Epoch 202/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 2.0712 - accuracy: 0.4774 - val_loss: 17.3231 - val_accuracy: 0.0762\n",
            "Epoch 203/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 2.0606 - accuracy: 0.4805 - val_loss: 17.2658 - val_accuracy: 0.0772\n",
            "Epoch 204/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 2.0575 - accuracy: 0.4806 - val_loss: 17.3623 - val_accuracy: 0.0754\n",
            "Epoch 205/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 2.0367 - accuracy: 0.4853 - val_loss: 17.4292 - val_accuracy: 0.0761\n",
            "Epoch 206/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 2.0407 - accuracy: 0.4821 - val_loss: 17.5024 - val_accuracy: 0.0774\n",
            "Epoch 207/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 2.0398 - accuracy: 0.4850 - val_loss: 17.5047 - val_accuracy: 0.0768\n",
            "Epoch 208/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 2.0428 - accuracy: 0.4857 - val_loss: 17.3029 - val_accuracy: 0.0748\n",
            "Epoch 209/400\n",
            "57424/57424 [==============================] - 27s 478us/step - loss: 2.0306 - accuracy: 0.4870 - val_loss: 17.4711 - val_accuracy: 0.0762\n",
            "Epoch 210/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 2.0303 - accuracy: 0.4878 - val_loss: 17.6686 - val_accuracy: 0.0773\n",
            "Epoch 211/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 2.0293 - accuracy: 0.4876 - val_loss: 17.6992 - val_accuracy: 0.0740\n",
            "Epoch 212/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 2.0168 - accuracy: 0.4907 - val_loss: 17.5800 - val_accuracy: 0.0767\n",
            "Epoch 213/400\n",
            "57424/57424 [==============================] - 27s 475us/step - loss: 2.0069 - accuracy: 0.4925 - val_loss: 17.8336 - val_accuracy: 0.0735\n",
            "Epoch 214/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 2.0119 - accuracy: 0.4903 - val_loss: 17.8113 - val_accuracy: 0.0758\n",
            "Epoch 215/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.9920 - accuracy: 0.4953 - val_loss: 17.7725 - val_accuracy: 0.0769\n",
            "Epoch 216/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 1.9968 - accuracy: 0.4929 - val_loss: 17.7465 - val_accuracy: 0.0740\n",
            "Epoch 217/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.9988 - accuracy: 0.4909 - val_loss: 17.8215 - val_accuracy: 0.0744\n",
            "Epoch 218/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.9957 - accuracy: 0.4949 - val_loss: 17.8061 - val_accuracy: 0.0770\n",
            "Epoch 219/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 1.9638 - accuracy: 0.4999 - val_loss: 17.9418 - val_accuracy: 0.0774\n",
            "Epoch 220/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.9799 - accuracy: 0.4982 - val_loss: 17.8380 - val_accuracy: 0.0747\n",
            "Epoch 221/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.9713 - accuracy: 0.5008 - val_loss: 17.7604 - val_accuracy: 0.0740\n",
            "Epoch 222/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 1.9761 - accuracy: 0.4968 - val_loss: 17.8346 - val_accuracy: 0.0736\n",
            "Epoch 223/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 1.9668 - accuracy: 0.5020 - val_loss: 17.7936 - val_accuracy: 0.0749\n",
            "Epoch 224/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 1.9662 - accuracy: 0.5013 - val_loss: 17.9895 - val_accuracy: 0.0748\n",
            "Epoch 225/400\n",
            "57424/57424 [==============================] - 27s 479us/step - loss: 1.9683 - accuracy: 0.5003 - val_loss: 17.9930 - val_accuracy: 0.0764\n",
            "Epoch 226/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.9527 - accuracy: 0.5025 - val_loss: 18.0142 - val_accuracy: 0.0756\n",
            "Epoch 227/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 1.9665 - accuracy: 0.5033 - val_loss: 17.9499 - val_accuracy: 0.0732\n",
            "Epoch 228/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 1.9408 - accuracy: 0.5042 - val_loss: 18.1195 - val_accuracy: 0.0761\n",
            "Epoch 229/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 1.9416 - accuracy: 0.5048 - val_loss: 18.3829 - val_accuracy: 0.0764\n",
            "Epoch 230/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.9279 - accuracy: 0.5073 - val_loss: 17.9729 - val_accuracy: 0.0724\n",
            "Epoch 231/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.9277 - accuracy: 0.5071 - val_loss: 18.3006 - val_accuracy: 0.0736\n",
            "Epoch 232/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 1.9305 - accuracy: 0.5078 - val_loss: 18.2143 - val_accuracy: 0.0737\n",
            "Epoch 233/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.9245 - accuracy: 0.5083 - val_loss: 18.1300 - val_accuracy: 0.0739\n",
            "Epoch 234/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.9115 - accuracy: 0.5096 - val_loss: 18.2893 - val_accuracy: 0.0733\n",
            "Epoch 235/400\n",
            "57424/57424 [==============================] - 27s 479us/step - loss: 1.9192 - accuracy: 0.5092 - val_loss: 18.4007 - val_accuracy: 0.0750\n",
            "Epoch 236/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.9045 - accuracy: 0.5131 - val_loss: 18.4679 - val_accuracy: 0.0749\n",
            "Epoch 237/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 1.9060 - accuracy: 0.5143 - val_loss: 18.4602 - val_accuracy: 0.0724\n",
            "Epoch 238/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 1.9113 - accuracy: 0.5105 - val_loss: 18.3329 - val_accuracy: 0.0739\n",
            "Epoch 239/400\n",
            "57424/57424 [==============================] - 28s 479us/step - loss: 1.9009 - accuracy: 0.5132 - val_loss: 18.6112 - val_accuracy: 0.0736\n",
            "Epoch 240/400\n",
            "57424/57424 [==============================] - 27s 477us/step - loss: 1.9044 - accuracy: 0.5136 - val_loss: 18.4059 - val_accuracy: 0.0733\n",
            "Epoch 241/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.9036 - accuracy: 0.5158 - val_loss: 18.4543 - val_accuracy: 0.0736\n",
            "Epoch 242/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 1.8767 - accuracy: 0.5197 - val_loss: 18.4934 - val_accuracy: 0.0722\n",
            "Epoch 243/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 1.8799 - accuracy: 0.5180 - val_loss: 18.4296 - val_accuracy: 0.0716\n",
            "Epoch 244/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 1.8795 - accuracy: 0.5210 - val_loss: 18.5637 - val_accuracy: 0.0724\n",
            "Epoch 245/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 1.8799 - accuracy: 0.5208 - val_loss: 18.7183 - val_accuracy: 0.0733\n",
            "Epoch 246/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 1.8809 - accuracy: 0.5193 - val_loss: 18.6360 - val_accuracy: 0.0736\n",
            "Epoch 247/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 1.8813 - accuracy: 0.5209 - val_loss: 18.3728 - val_accuracy: 0.0719\n",
            "Epoch 248/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.8715 - accuracy: 0.5195 - val_loss: 18.5906 - val_accuracy: 0.0727\n",
            "Epoch 249/400\n",
            "57424/57424 [==============================] - 27s 475us/step - loss: 1.8619 - accuracy: 0.5222 - val_loss: 18.6119 - val_accuracy: 0.0739\n",
            "Epoch 250/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.8514 - accuracy: 0.5233 - val_loss: 18.6940 - val_accuracy: 0.0713\n",
            "Epoch 251/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 1.8670 - accuracy: 0.5209 - val_loss: 18.6119 - val_accuracy: 0.0719\n",
            "Epoch 252/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.8657 - accuracy: 0.5234 - val_loss: 18.6322 - val_accuracy: 0.0705\n",
            "Epoch 253/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.8617 - accuracy: 0.5220 - val_loss: 18.7616 - val_accuracy: 0.0729\n",
            "Epoch 254/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 1.8521 - accuracy: 0.5264 - val_loss: 18.7421 - val_accuracy: 0.0742\n",
            "Epoch 255/400\n",
            "57424/57424 [==============================] - 29s 501us/step - loss: 1.8362 - accuracy: 0.5281 - val_loss: 18.6784 - val_accuracy: 0.0721\n",
            "Epoch 256/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.8453 - accuracy: 0.5254 - val_loss: 18.8891 - val_accuracy: 0.0739\n",
            "Epoch 257/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.8485 - accuracy: 0.5282 - val_loss: 18.6402 - val_accuracy: 0.0723\n",
            "Epoch 258/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.8418 - accuracy: 0.5284 - val_loss: 18.7033 - val_accuracy: 0.0723\n",
            "Epoch 259/400\n",
            "57424/57424 [==============================] - 27s 477us/step - loss: 1.8305 - accuracy: 0.5297 - val_loss: 18.8409 - val_accuracy: 0.0702\n",
            "Epoch 260/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.8363 - accuracy: 0.5303 - val_loss: 19.0160 - val_accuracy: 0.0763\n",
            "Epoch 261/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 1.8245 - accuracy: 0.5309 - val_loss: 18.9185 - val_accuracy: 0.0708\n",
            "Epoch 262/400\n",
            "57424/57424 [==============================] - 29s 500us/step - loss: 1.8138 - accuracy: 0.5325 - val_loss: 19.2485 - val_accuracy: 0.0711\n",
            "Epoch 263/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.8200 - accuracy: 0.5310 - val_loss: 18.9933 - val_accuracy: 0.0701\n",
            "Epoch 264/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 1.8069 - accuracy: 0.5337 - val_loss: 19.1477 - val_accuracy: 0.0710\n",
            "Epoch 265/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 1.8080 - accuracy: 0.5351 - val_loss: 19.1586 - val_accuracy: 0.0707\n",
            "Epoch 266/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.8084 - accuracy: 0.5367 - val_loss: 19.0590 - val_accuracy: 0.0725\n",
            "Epoch 267/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.8146 - accuracy: 0.5347 - val_loss: 19.2746 - val_accuracy: 0.0722\n",
            "Epoch 268/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 1.8047 - accuracy: 0.5350 - val_loss: 19.1073 - val_accuracy: 0.0693\n",
            "Epoch 269/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 1.7919 - accuracy: 0.5418 - val_loss: 19.1181 - val_accuracy: 0.0708\n",
            "Epoch 270/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 1.8063 - accuracy: 0.5353 - val_loss: 19.1653 - val_accuracy: 0.0706\n",
            "Epoch 271/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 1.7817 - accuracy: 0.5417 - val_loss: 19.0870 - val_accuracy: 0.0702\n",
            "Epoch 272/400\n",
            "57424/57424 [==============================] - 29s 502us/step - loss: 1.7913 - accuracy: 0.5395 - val_loss: 19.2051 - val_accuracy: 0.0702\n",
            "Epoch 273/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 1.7952 - accuracy: 0.5364 - val_loss: 19.2692 - val_accuracy: 0.0718\n",
            "Epoch 274/400\n",
            "57424/57424 [==============================] - 27s 471us/step - loss: 1.7912 - accuracy: 0.5403 - val_loss: 19.2294 - val_accuracy: 0.0716\n",
            "Epoch 275/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.7782 - accuracy: 0.5418 - val_loss: 19.2383 - val_accuracy: 0.0694\n",
            "Epoch 276/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.7745 - accuracy: 0.5401 - val_loss: 19.3695 - val_accuracy: 0.0706\n",
            "Epoch 277/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.7805 - accuracy: 0.5434 - val_loss: 19.1343 - val_accuracy: 0.0714\n",
            "Epoch 278/400\n",
            "57424/57424 [==============================] - 29s 508us/step - loss: 1.7779 - accuracy: 0.5429 - val_loss: 19.2036 - val_accuracy: 0.0681\n",
            "Epoch 279/400\n",
            "57424/57424 [==============================] - 29s 508us/step - loss: 1.7792 - accuracy: 0.5414 - val_loss: 19.4467 - val_accuracy: 0.0726\n",
            "Epoch 280/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 1.7610 - accuracy: 0.5436 - val_loss: 19.3723 - val_accuracy: 0.0694\n",
            "Epoch 281/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 1.7621 - accuracy: 0.5456 - val_loss: 19.2906 - val_accuracy: 0.0679\n",
            "Epoch 282/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.7653 - accuracy: 0.5456 - val_loss: 19.2215 - val_accuracy: 0.0705\n",
            "Epoch 283/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 1.7588 - accuracy: 0.5469 - val_loss: 19.3393 - val_accuracy: 0.0710\n",
            "Epoch 284/400\n",
            "57424/57424 [==============================] - 27s 473us/step - loss: 1.7707 - accuracy: 0.5431 - val_loss: 19.2053 - val_accuracy: 0.0693\n",
            "Epoch 285/400\n",
            "57424/57424 [==============================] - 28s 489us/step - loss: 1.7542 - accuracy: 0.5477 - val_loss: 19.4481 - val_accuracy: 0.0681\n",
            "Epoch 286/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.7470 - accuracy: 0.5472 - val_loss: 19.3854 - val_accuracy: 0.0692\n",
            "Epoch 287/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.7509 - accuracy: 0.5485 - val_loss: 19.2792 - val_accuracy: 0.0679\n",
            "Epoch 288/400\n",
            "57424/57424 [==============================] - 27s 475us/step - loss: 1.7401 - accuracy: 0.5512 - val_loss: 19.5833 - val_accuracy: 0.0709\n",
            "Epoch 289/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.7314 - accuracy: 0.5510 - val_loss: 19.5247 - val_accuracy: 0.0686\n",
            "Epoch 290/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.7346 - accuracy: 0.5486 - val_loss: 19.5035 - val_accuracy: 0.0690\n",
            "Epoch 291/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 1.7294 - accuracy: 0.5536 - val_loss: 19.6286 - val_accuracy: 0.0689\n",
            "Epoch 292/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.7400 - accuracy: 0.5495 - val_loss: 19.5443 - val_accuracy: 0.0687\n",
            "Epoch 293/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.7402 - accuracy: 0.5502 - val_loss: 19.5953 - val_accuracy: 0.0703\n",
            "Epoch 294/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.7413 - accuracy: 0.5508 - val_loss: 19.4617 - val_accuracy: 0.0678\n",
            "Epoch 295/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.7335 - accuracy: 0.5534 - val_loss: 19.7065 - val_accuracy: 0.0687\n",
            "Epoch 296/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.7274 - accuracy: 0.5538 - val_loss: 19.6489 - val_accuracy: 0.0695\n",
            "Epoch 297/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 1.7247 - accuracy: 0.5539 - val_loss: 19.6937 - val_accuracy: 0.0687\n",
            "Epoch 298/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 1.7338 - accuracy: 0.5510 - val_loss: 19.6061 - val_accuracy: 0.0713\n",
            "Epoch 299/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 1.7312 - accuracy: 0.5530 - val_loss: 19.5512 - val_accuracy: 0.0687\n",
            "Epoch 300/400\n",
            "57424/57424 [==============================] - 29s 498us/step - loss: 1.7261 - accuracy: 0.5541 - val_loss: 19.4025 - val_accuracy: 0.0668\n",
            "Epoch 301/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 1.7114 - accuracy: 0.5585 - val_loss: 19.6951 - val_accuracy: 0.0679\n",
            "Epoch 302/400\n",
            "57424/57424 [==============================] - 29s 501us/step - loss: 1.7079 - accuracy: 0.5567 - val_loss: 19.5899 - val_accuracy: 0.0703\n",
            "Epoch 303/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 1.7155 - accuracy: 0.5566 - val_loss: 19.7657 - val_accuracy: 0.0692\n",
            "Epoch 304/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 1.7250 - accuracy: 0.5571 - val_loss: 19.6142 - val_accuracy: 0.0675\n",
            "Epoch 305/400\n",
            "57424/57424 [==============================] - 27s 477us/step - loss: 1.7160 - accuracy: 0.5557 - val_loss: 19.6248 - val_accuracy: 0.0661\n",
            "Epoch 306/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.7029 - accuracy: 0.5595 - val_loss: 19.6764 - val_accuracy: 0.0671\n",
            "Epoch 307/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.6995 - accuracy: 0.5578 - val_loss: 19.8039 - val_accuracy: 0.0665\n",
            "Epoch 308/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.7149 - accuracy: 0.5560 - val_loss: 19.5201 - val_accuracy: 0.0670\n",
            "Epoch 309/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.7134 - accuracy: 0.5555 - val_loss: 19.7128 - val_accuracy: 0.0686\n",
            "Epoch 310/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.6934 - accuracy: 0.5603 - val_loss: 19.7361 - val_accuracy: 0.0662\n",
            "Epoch 311/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.6949 - accuracy: 0.5608 - val_loss: 19.9224 - val_accuracy: 0.0682\n",
            "Epoch 312/400\n",
            "57424/57424 [==============================] - 27s 474us/step - loss: 1.6845 - accuracy: 0.5614 - val_loss: 19.8989 - val_accuracy: 0.0687\n",
            "Epoch 313/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.6848 - accuracy: 0.5644 - val_loss: 19.6570 - val_accuracy: 0.0667\n",
            "Epoch 314/400\n",
            "57424/57424 [==============================] - 27s 475us/step - loss: 1.6996 - accuracy: 0.5611 - val_loss: 19.9519 - val_accuracy: 0.0680\n",
            "Epoch 315/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.6742 - accuracy: 0.5666 - val_loss: 19.8091 - val_accuracy: 0.0651\n",
            "Epoch 316/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.6873 - accuracy: 0.5650 - val_loss: 19.8770 - val_accuracy: 0.0672\n",
            "Epoch 317/400\n",
            "57424/57424 [==============================] - 27s 477us/step - loss: 1.6799 - accuracy: 0.5632 - val_loss: 19.9540 - val_accuracy: 0.0680\n",
            "Epoch 318/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 1.6658 - accuracy: 0.5669 - val_loss: 19.7924 - val_accuracy: 0.0662\n",
            "Epoch 319/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 1.6573 - accuracy: 0.5681 - val_loss: 20.0368 - val_accuracy: 0.0665\n",
            "Epoch 320/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.6733 - accuracy: 0.5675 - val_loss: 20.0029 - val_accuracy: 0.0659\n",
            "Epoch 321/400\n",
            "57424/57424 [==============================] - 27s 478us/step - loss: 1.6437 - accuracy: 0.5739 - val_loss: 20.2777 - val_accuracy: 0.0675\n",
            "Epoch 322/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.6698 - accuracy: 0.5659 - val_loss: 19.9398 - val_accuracy: 0.0665\n",
            "Epoch 323/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 1.6722 - accuracy: 0.5652 - val_loss: 20.0664 - val_accuracy: 0.0677\n",
            "Epoch 324/400\n",
            "57424/57424 [==============================] - 29s 502us/step - loss: 1.6645 - accuracy: 0.5682 - val_loss: 20.0261 - val_accuracy: 0.0658\n",
            "Epoch 325/400\n",
            "57424/57424 [==============================] - 28s 489us/step - loss: 1.6621 - accuracy: 0.5704 - val_loss: 19.9123 - val_accuracy: 0.0634\n",
            "Epoch 326/400\n",
            "57424/57424 [==============================] - 28s 495us/step - loss: 1.6676 - accuracy: 0.5670 - val_loss: 20.1597 - val_accuracy: 0.0664\n",
            "Epoch 327/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.6579 - accuracy: 0.5679 - val_loss: 20.0820 - val_accuracy: 0.0677\n",
            "Epoch 328/400\n",
            "57424/57424 [==============================] - 29s 501us/step - loss: 1.6536 - accuracy: 0.5721 - val_loss: 20.3017 - val_accuracy: 0.0678\n",
            "Epoch 329/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 1.6666 - accuracy: 0.5690 - val_loss: 20.0406 - val_accuracy: 0.0672\n",
            "Epoch 330/400\n",
            "57424/57424 [==============================] - 28s 486us/step - loss: 1.6589 - accuracy: 0.5704 - val_loss: 19.9457 - val_accuracy: 0.0673\n",
            "Epoch 331/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.6511 - accuracy: 0.5732 - val_loss: 20.1894 - val_accuracy: 0.0652\n",
            "Epoch 332/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.6411 - accuracy: 0.5749 - val_loss: 20.1014 - val_accuracy: 0.0676\n",
            "Epoch 333/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.6447 - accuracy: 0.5740 - val_loss: 20.3185 - val_accuracy: 0.0681\n",
            "Epoch 334/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 1.6435 - accuracy: 0.5725 - val_loss: 20.4516 - val_accuracy: 0.0688\n",
            "Epoch 335/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.6312 - accuracy: 0.5749 - val_loss: 20.3535 - val_accuracy: 0.0659\n",
            "Epoch 336/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.6317 - accuracy: 0.5737 - val_loss: 20.2283 - val_accuracy: 0.0678\n",
            "Epoch 337/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 1.6391 - accuracy: 0.5741 - val_loss: 20.3588 - val_accuracy: 0.0662\n",
            "Epoch 338/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.6301 - accuracy: 0.5752 - val_loss: 20.5324 - val_accuracy: 0.0678\n",
            "Epoch 339/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.6521 - accuracy: 0.5698 - val_loss: 20.2409 - val_accuracy: 0.0668\n",
            "Epoch 340/400\n",
            "57424/57424 [==============================] - 27s 478us/step - loss: 1.6468 - accuracy: 0.5712 - val_loss: 20.3989 - val_accuracy: 0.0669\n",
            "Epoch 341/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 1.6475 - accuracy: 0.5726 - val_loss: 20.2304 - val_accuracy: 0.0661\n",
            "Epoch 342/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.6290 - accuracy: 0.5762 - val_loss: 20.2901 - val_accuracy: 0.0648\n",
            "Epoch 343/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.6276 - accuracy: 0.5767 - val_loss: 20.3704 - val_accuracy: 0.0663\n",
            "Epoch 344/400\n",
            "57424/57424 [==============================] - 27s 475us/step - loss: 1.6225 - accuracy: 0.5764 - val_loss: 20.4793 - val_accuracy: 0.0661\n",
            "Epoch 345/400\n",
            "57424/57424 [==============================] - 27s 477us/step - loss: 1.6336 - accuracy: 0.5779 - val_loss: 20.2446 - val_accuracy: 0.0661\n",
            "Epoch 346/400\n",
            "57424/57424 [==============================] - 27s 479us/step - loss: 1.6131 - accuracy: 0.5817 - val_loss: 20.4063 - val_accuracy: 0.0648\n",
            "Epoch 347/400\n",
            "57424/57424 [==============================] - 27s 477us/step - loss: 1.6272 - accuracy: 0.5767 - val_loss: 20.3798 - val_accuracy: 0.0653\n",
            "Epoch 348/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.6258 - accuracy: 0.5771 - val_loss: 20.3606 - val_accuracy: 0.0649\n",
            "Epoch 349/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.6067 - accuracy: 0.5812 - val_loss: 20.5160 - val_accuracy: 0.0652\n",
            "Epoch 350/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.6037 - accuracy: 0.5824 - val_loss: 20.4648 - val_accuracy: 0.0648\n",
            "Epoch 351/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.6099 - accuracy: 0.5809 - val_loss: 20.3372 - val_accuracy: 0.0629\n",
            "Epoch 352/400\n",
            "57424/57424 [==============================] - 29s 509us/step - loss: 1.6273 - accuracy: 0.5792 - val_loss: 20.2387 - val_accuracy: 0.0653\n",
            "Epoch 353/400\n",
            "57424/57424 [==============================] - 28s 496us/step - loss: 1.6021 - accuracy: 0.5831 - val_loss: 20.7667 - val_accuracy: 0.0663\n",
            "Epoch 354/400\n",
            "57424/57424 [==============================] - 28s 487us/step - loss: 1.6036 - accuracy: 0.5839 - val_loss: 20.6826 - val_accuracy: 0.0666\n",
            "Epoch 355/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.5930 - accuracy: 0.5836 - val_loss: 20.7143 - val_accuracy: 0.0662\n",
            "Epoch 356/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 1.6062 - accuracy: 0.5829 - val_loss: 20.4542 - val_accuracy: 0.0637\n",
            "Epoch 357/400\n",
            "57424/57424 [==============================] - 28s 479us/step - loss: 1.6254 - accuracy: 0.5795 - val_loss: 20.3800 - val_accuracy: 0.0660\n",
            "Epoch 358/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.6057 - accuracy: 0.5831 - val_loss: 20.4738 - val_accuracy: 0.0650\n",
            "Epoch 359/400\n",
            "57424/57424 [==============================] - 28s 489us/step - loss: 1.5930 - accuracy: 0.5865 - val_loss: 20.5447 - val_accuracy: 0.0645\n",
            "Epoch 360/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.5915 - accuracy: 0.5858 - val_loss: 20.7345 - val_accuracy: 0.0642\n",
            "Epoch 361/400\n",
            "57424/57424 [==============================] - 28s 479us/step - loss: 1.5899 - accuracy: 0.5868 - val_loss: 20.6195 - val_accuracy: 0.0647\n",
            "Epoch 362/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.6092 - accuracy: 0.5837 - val_loss: 20.4047 - val_accuracy: 0.0643\n",
            "Epoch 363/400\n",
            "57424/57424 [==============================] - 27s 474us/step - loss: 1.6006 - accuracy: 0.5844 - val_loss: 20.7298 - val_accuracy: 0.0668\n",
            "Epoch 364/400\n",
            "57424/57424 [==============================] - 27s 478us/step - loss: 1.5922 - accuracy: 0.5849 - val_loss: 20.9258 - val_accuracy: 0.0668\n",
            "Epoch 365/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.6007 - accuracy: 0.5839 - val_loss: 20.6054 - val_accuracy: 0.0620\n",
            "Epoch 366/400\n",
            "57424/57424 [==============================] - 28s 481us/step - loss: 1.5902 - accuracy: 0.5876 - val_loss: 20.6657 - val_accuracy: 0.0651\n",
            "Epoch 367/400\n",
            "57424/57424 [==============================] - 29s 499us/step - loss: 1.5770 - accuracy: 0.5875 - val_loss: 20.6038 - val_accuracy: 0.0640\n",
            "Epoch 368/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.5899 - accuracy: 0.5861 - val_loss: 20.8412 - val_accuracy: 0.0657\n",
            "Epoch 369/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.5838 - accuracy: 0.5868 - val_loss: 20.7359 - val_accuracy: 0.0656\n",
            "Epoch 370/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.5719 - accuracy: 0.5920 - val_loss: 20.7129 - val_accuracy: 0.0659\n",
            "Epoch 371/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.5763 - accuracy: 0.5882 - val_loss: 20.5170 - val_accuracy: 0.0628\n",
            "Epoch 372/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.5753 - accuracy: 0.5902 - val_loss: 20.8743 - val_accuracy: 0.0660\n",
            "Epoch 373/400\n",
            "57424/57424 [==============================] - 28s 488us/step - loss: 1.5793 - accuracy: 0.5854 - val_loss: 20.8065 - val_accuracy: 0.0641\n",
            "Epoch 374/400\n",
            "57424/57424 [==============================] - 28s 482us/step - loss: 1.5893 - accuracy: 0.5890 - val_loss: 20.5661 - val_accuracy: 0.0647\n",
            "Epoch 375/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.5763 - accuracy: 0.5917 - val_loss: 20.6157 - val_accuracy: 0.0630\n",
            "Epoch 376/400\n",
            "57424/57424 [==============================] - 29s 503us/step - loss: 1.5903 - accuracy: 0.5864 - val_loss: 20.7813 - val_accuracy: 0.0658\n",
            "Epoch 377/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 1.5624 - accuracy: 0.5927 - val_loss: 20.7485 - val_accuracy: 0.0657\n",
            "Epoch 378/400\n",
            "57424/57424 [==============================] - 28s 483us/step - loss: 1.5767 - accuracy: 0.5909 - val_loss: 20.7996 - val_accuracy: 0.0649\n",
            "Epoch 379/400\n",
            "57424/57424 [==============================] - 28s 489us/step - loss: 1.5558 - accuracy: 0.5906 - val_loss: 20.8588 - val_accuracy: 0.0641\n",
            "Epoch 380/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 1.5692 - accuracy: 0.5909 - val_loss: 20.9216 - val_accuracy: 0.0649\n",
            "Epoch 381/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.5692 - accuracy: 0.5896 - val_loss: 21.1397 - val_accuracy: 0.0682\n",
            "Epoch 382/400\n",
            "57424/57424 [==============================] - 27s 478us/step - loss: 1.5756 - accuracy: 0.5934 - val_loss: 20.6481 - val_accuracy: 0.0639\n",
            "Epoch 383/400\n",
            "57424/57424 [==============================] - 28s 484us/step - loss: 1.5715 - accuracy: 0.5934 - val_loss: 20.7910 - val_accuracy: 0.0647\n",
            "Epoch 384/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.5632 - accuracy: 0.5938 - val_loss: 20.6516 - val_accuracy: 0.0663\n",
            "Epoch 385/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.5443 - accuracy: 0.5943 - val_loss: 20.9184 - val_accuracy: 0.0637\n",
            "Epoch 386/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.5612 - accuracy: 0.5951 - val_loss: 20.7842 - val_accuracy: 0.0633\n",
            "Epoch 387/400\n",
            "57424/57424 [==============================] - 29s 497us/step - loss: 1.5478 - accuracy: 0.5967 - val_loss: 20.8514 - val_accuracy: 0.0648\n",
            "Epoch 388/400\n",
            "57424/57424 [==============================] - 28s 480us/step - loss: 1.5522 - accuracy: 0.5947 - val_loss: 21.0000 - val_accuracy: 0.0652\n",
            "Epoch 389/400\n",
            "57424/57424 [==============================] - 27s 477us/step - loss: 1.5511 - accuracy: 0.5952 - val_loss: 20.9657 - val_accuracy: 0.0637\n",
            "Epoch 390/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 1.5516 - accuracy: 0.5969 - val_loss: 21.0509 - val_accuracy: 0.0638\n",
            "Epoch 391/400\n",
            "57424/57424 [==============================] - 28s 493us/step - loss: 1.5542 - accuracy: 0.5950 - val_loss: 20.9298 - val_accuracy: 0.0651\n",
            "Epoch 392/400\n",
            "57424/57424 [==============================] - 28s 485us/step - loss: 1.5406 - accuracy: 0.5958 - val_loss: 20.9579 - val_accuracy: 0.0624\n",
            "Epoch 393/400\n",
            "57424/57424 [==============================] - 27s 473us/step - loss: 1.5344 - accuracy: 0.5989 - val_loss: 21.1571 - val_accuracy: 0.0638\n",
            "Epoch 394/400\n",
            "57424/57424 [==============================] - 28s 492us/step - loss: 1.5523 - accuracy: 0.5940 - val_loss: 20.8801 - val_accuracy: 0.0629\n",
            "Epoch 395/400\n",
            "57424/57424 [==============================] - 28s 490us/step - loss: 1.5781 - accuracy: 0.5911 - val_loss: 20.8180 - val_accuracy: 0.0640\n",
            "Epoch 396/400\n",
            "57424/57424 [==============================] - 27s 475us/step - loss: 1.5551 - accuracy: 0.5953 - val_loss: 20.9822 - val_accuracy: 0.0655\n",
            "Epoch 397/400\n",
            "57424/57424 [==============================] - 27s 476us/step - loss: 1.5380 - accuracy: 0.5970 - val_loss: 21.0988 - val_accuracy: 0.0655\n",
            "Epoch 398/400\n",
            "57424/57424 [==============================] - 28s 494us/step - loss: 1.5425 - accuracy: 0.5993 - val_loss: 20.8903 - val_accuracy: 0.0657\n",
            "Epoch 399/400\n",
            "57424/57424 [==============================] - 28s 489us/step - loss: 1.5323 - accuracy: 0.6007 - val_loss: 21.0298 - val_accuracy: 0.0656\n",
            "Epoch 400/400\n",
            "57424/57424 [==============================] - 28s 491us/step - loss: 1.5331 - accuracy: 0.6030 - val_loss: 21.1665 - val_accuracy: 0.0650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcG_I-aciDAh",
        "colab_type": "text"
      },
      "source": [
        "# Load Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqyJ_3KkS4Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/accuracy.txt\", \"w\") as file:\n",
        "    file.write(str(train_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbLOwh1_S4Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/loss.txt\", \"w\") as file:\n",
        "    file.write(str(train_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPF9NU4Aj09x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/val_accuracy.txt\", \"w\") as file:\n",
        "    file.write(str(val_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrs9UO4LdZLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/gdrive/My Drive/Colab Notebooks/Dataset/Next text/val_loss.txt\", \"w\") as file:\n",
        "    file.write(str(val_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwsM5iTXWMkc",
        "colab_type": "text"
      },
      "source": [
        "# Plot loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGNuk2kYWMWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhtOD3iLWKDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "575c2966-8751-49ed-fb58-13807d2d0b3f"
      },
      "source": [
        "def plot_accuracy(data2):\n",
        "    x = list(range(1, len(data2) + 1))\n",
        "    plt.plot(x, data2, label = 'Training accuracy')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Epoch')  \n",
        "    \n",
        "plot_accuracy(train_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_i21Hg4WPh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "54a0abb7-9848-490a-f8cf-3784ba058699"
      },
      "source": [
        "def plot_loss(data1):\n",
        "    x = list(range(1, len(data1) + 1))\n",
        "    plt.plot(x, data1, label = 'Training Loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss vs. Epoch')  \n",
        "    \n",
        "plot_loss(train_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FisVMd-1Wfup",
        "colab_type": "text"
      },
      "source": [
        "# Generating New Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8ImbyuCseC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsrtXv8zsgo8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "662b529a-f9b4-4a73-83cd-89cab8127537"
      },
      "source": [
        "model=load_model('/gdrive/My Drive/Colab Notebooks/Dataset/Next text/text_generation.h5')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPDAjGpytp88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/gdrive/My Drive/Colab Notebooks/Dataset/Next text/300620.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if3UgTw0Whcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#from pickle import load\n",
        "#from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7vDlg03Wia7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
        "    \n",
        "    #INPUTS:\n",
        "    #model : model that was trained on text data\n",
        "    #tokenizer : tokenizer that was fit on text data\n",
        "    #seq_len : length of training sequence\n",
        "    #seed_text : raw string text to serve as the seed\n",
        "    #num_gen_words : number of words to be generated by model\n",
        "    \n",
        "    \n",
        "    # Final Output\n",
        "    output_text = []\n",
        "    \n",
        "    # Intial Seed Sequence\n",
        "    input_text = seed_text\n",
        "    \n",
        "    # Create num_gen_words\n",
        "    for i in range(num_gen_words):\n",
        "        \n",
        "        # Take the input text string and encode it to a sequence\n",
        "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        \n",
        "        # Pad sequences to our trained rate (30 words in the video)\n",
        "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "        \n",
        "        # Predict Class Probabilities for each word\n",
        "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
        "        \n",
        "        # Grab word\n",
        "        pred_word = tokenizer.index_word[pred_word_ind] \n",
        "        \n",
        "        # Update the sequence of input text (shifting one over with the new word)\n",
        "        input_text += ' ' + pred_word\n",
        "        \n",
        "        output_text.append(pred_word)\n",
        "        \n",
        "    # Make it look like a sentence.\n",
        "    return ' '.join(output_text)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSo43X1fWkoZ",
        "colab_type": "text"
      },
      "source": [
        "### Predict next sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB5CIfD-vS4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20edd10e-9a02-4f91-ade5-d4c0a848e718"
      },
      "source": [
        "x= ' '.join(text_sequences[1000])\n",
        "x"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'hadnot even remembered that today happened to be harry ’s twelfth birthday of course his hopes hadnot been high they had never given him real present let alone cake — but'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qusel1uWWjKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_seed_text = x"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnQP59brWmDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d93387a8-a718-4c76-fad5-86173c5d0e77"
      },
      "source": [
        "random_seed_text"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'hadnot even remembered that today happened to be harry ’s twelfth birthday of course his hopes hadnot been high they had never given him real present let alone cake — but'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzd2c31HWm55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_text = ' '.join(random_seed_text)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq5ghFdTWo0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e771c851-8df8-4fa1-c29d-a343e21241d8"
      },
      "source": [
        "print(seed_text)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h a d n o t   e v e n   r e m e m b e r e d   t h a t   t o d a y   h a p p e n e d   t o   b e   h a r r y   ’ s   t w e l f t h   b i r t h d a y   o f   c o u r s e   h i s   h o p e s   h a d n o t   b e e n   h i g h   t h e y   h a d   n e v e r   g i v e n   h i m   r e a l   p r e s e n t   l e t   a l o n e   c a k e   —   b u t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQMxpI8SWpxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e546267b-c793-4638-c8b6-2ea4fa5a3556"
      },
      "source": [
        "print(generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=24))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "who was having “ what wouldnot have worry ” ron shouted “ lie to my wand ” said riddle drawing sun her hand impatiently\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtPnvNx7Gusq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
